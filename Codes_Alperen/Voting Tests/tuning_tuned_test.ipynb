{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.optimizers import Adam\n",
    "import torch.optim as optim\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, cohen_kappa_score\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "SEED = 42\n",
    "n_splits = 5\n",
    "\n",
    "# Custom Scorer for Cohen Kappa with Quadratic Weights\n",
    "def cohen_kappa_quadratic(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom scorer for Cohen's Kappa with quadratic weights.\n",
    "    Rounds predictions to the nearest integer before calculation.\n",
    "    \"\"\"\n",
    "    y_pred_rounded = y_pred.astype(int)\n",
    "    y_true_rounded = y_true.astype(int)\n",
    "    return cohen_kappa_score(y_true_rounded, y_pred_rounded, weights='quadratic')\n",
    "\n",
    "# Wrapping the custom scorer\n",
    "kappa_scorer = make_scorer(cohen_kappa_quadratic, greater_is_better=True)\n",
    "\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, encoding_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*3, encoding_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(encoding_dim*2, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, input_dim*2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*2, input_dim*3),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(input_dim*3, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "\n",
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    data_tensor = torch.FloatTensor(df_scaled)\n",
    "    \n",
    "    input_dim = data_tensor.shape[1]\n",
    "    autoencoder = AutoEncoder(input_dim, encoding_dim)\n",
    "    \n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(autoencoder.parameters())\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        for i in range(0, len(data_tensor), batch_size):\n",
    "            batch = data_tensor[i : i + batch_size]\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed = autoencoder(batch)\n",
    "            loss = criterion(reconstructed, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n",
    "                 \n",
    "    with torch.no_grad():\n",
    "        encoded_data = autoencoder.encoder(data_tensor).numpy()\n",
    "        \n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "def feature_engineering(df):\n",
    "    season_cols = [col for col in df.columns if 'Season' in col]\n",
    "    df = df.drop(season_cols, axis=1) \n",
    "    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n",
    "    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n",
    "    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n",
    "    \n",
    "    return df\n",
    "\n",
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "    return df.describe().values.reshape(-1), filename.split('=')[1]\n",
    "\n",
    "def map_to_scaled_real(y_pred):\n",
    "    \"\"\"\n",
    "    Map values from 0-100 to a smaller real number with scaling in specific ranges.\n",
    "\n",
    "    Args:\n",
    "    - y_pred (array-like): Input values ranging from 0 to 100.\n",
    "\n",
    "    Returns:\n",
    "    - Scaled values as per the mapping logic.\n",
    "    \"\"\"\n",
    "    # Initialize the result array\n",
    "    scaled = np.zeros_like(y_pred, dtype=float)\n",
    "\n",
    "    # Apply the mapping logic\n",
    "    scaled = np.where((y_pred >= 0) & (y_pred <= 30), \n",
    "                      y_pred / 30,  # Scale 0-30 to 0-1\n",
    "                      scaled)\n",
    "    \n",
    "    scaled = np.where((y_pred > 30) & (y_pred <= 49),\n",
    "                      1 + (y_pred - 31) / (49 - 31),  # Scale 31-49 to 1-2\n",
    "                      scaled)\n",
    "    \n",
    "    scaled = np.where((y_pred > 50) & (y_pred <= 79),\n",
    "                      2 + (y_pred - 51) / (79 - 51),  # Scale 51-79 to 2-3\n",
    "                      scaled)\n",
    "    \n",
    "    scaled = np.where((y_pred > 80) & (y_pred <= 100),\n",
    "                      3 + (y_pred - 81) / (100 - 81),  # Scale 81-100 to 3-4\n",
    "                      scaled)\n",
    "\n",
    "    return scaled\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    stats, indexes = zip(*results)\n",
    "    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n",
    "    df['id'] = indexes\n",
    "    return df\n",
    "\n",
    "# Hyperparameter tuning\n",
    "def tune_model(regressor, param_grid, X, y):\n",
    "    pipeline = Pipeline(steps=[('imputer', SimpleImputer(strategy='median')), ('regressor', regressor)])\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        scoring=kappa_scorer,\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=2\n",
    "    )\n",
    "    grid_search.fit(X, y)\n",
    "    print(f\"Best Parameters for {regressor.__class__.__name__}: {grid_search.best_params_}\")\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "# Load datasets\n",
    "train = pd.read_csv('/Users/ad53533/Desktop/Applied ML/Project/train.csv')\n",
    "test = pd.read_csv('/Users/ad53533/Desktop/Applied ML/Project/test.csv')\n",
    "sample = pd.read_csv('/Users/ad53533/Desktop/Applied ML/Project/sample_submission.csv')\n",
    "\n",
    "# train_ts = load_time_series(\"/Users/ad53533/Desktop/Applied ML/Project/series_train.parquet\")\n",
    "# test_ts = load_time_series(\"/Users/ad53533/Desktop/Applied ML/Project/series_test.parquet\")\n",
    "# \n",
    "# df_train = train_ts.drop('id', axis=1)\n",
    "# df_test = test_ts.drop('id', axis=1)\n",
    "# \n",
    "# train_ts_encoded = perform_autoencoder(df_train, encoding_dim=30, epochs=100, batch_size=32)\n",
    "# test_ts_encoded = perform_autoencoder(df_test, encoding_dim=30, epochs=100, batch_size=32)\n",
    "\n",
    "# time_series_cols = train_ts_encoded.columns.tolist()\n",
    "# train_ts_encoded[\"id\"]=train_ts[\"id\"]\n",
    "# test_ts_encoded['id']=test_ts[\"id\"]\n",
    "\n",
    "# train = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n",
    "# test = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputed_data = imputer.fit_transform(train[numeric_cols])\n",
    "train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n",
    "train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n",
    "for col in train.columns:\n",
    "    if col not in numeric_cols:\n",
    "        train_imputed[col] = train[col]\n",
    "        \n",
    "train = train_imputed\n",
    "\n",
    "train = feature_engineering(train)\n",
    "train = train.dropna(thresh=10, axis=0)\n",
    "test = feature_engineering(test)\n",
    "\n",
    "train = train.drop('id', axis=1)\n",
    "test  = test .drop('id', axis=1)   \n",
    "\n",
    "\n",
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Frame_num',\n",
    "                'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'sii', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours']\n",
    "\n",
    "\n",
    "# featuresCols += time_series_cols\n",
    "\n",
    "train = train.dropna(subset='sii')\n",
    "train_PCIAT = train['PCIAT-PCIAT_Total']\n",
    "train = train[featuresCols]\n",
    "train_sii_mapped = map_to_scaled_real(train_PCIAT)\n",
    "\n",
    "featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n",
    "                'CGAS-CGAS_Score', 'Physical-BMI',\n",
    "                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n",
    "                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n",
    "                'Fitness_Endurance-Max_Stage',\n",
    "                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n",
    "                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n",
    "                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n",
    "                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n",
    "                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n",
    "                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n",
    "                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Frame_num',\n",
    "                'PAQ_A-PAQ_A_Total',\n",
    "                'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n",
    "                'SDS-SDS_Total_T',\n",
    "                'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours']\n",
    "\n",
    "# featuresCols += time_series_cols\n",
    "test = test[featuresCols]\n",
    "if np.any(np.isinf(train)):\n",
    "    train = train.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "def quadratic_weighted_kappa(y_true, y_pred):\n",
    "    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n",
    "\n",
    "def threshold_Rounder(oof_non_rounded, thresholds):\n",
    "    return np.where(oof_non_rounded < thresholds[0], 0,\n",
    "                    np.where(oof_non_rounded < thresholds[1], 1,\n",
    "                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n",
    "\n",
    "def evaluate_predictions(thresholds, y_true, oof_non_rounded):\n",
    "    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n",
    "    return -quadratic_weighted_kappa(y_true, rounded_p)\n",
    "\n",
    "def TrainML(model_class, test_data):\n",
    "    X = train.drop(['sii'], axis=1)\n",
    "    y = train['sii']\n",
    "\n",
    "    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n",
    "    \n",
    "    train_S = []\n",
    "    test_S = []\n",
    "    \n",
    "    oof_non_rounded = np.zeros(len(y), dtype=float) \n",
    "    oof_rounded = np.zeros(len(y), dtype=int) \n",
    "    test_preds = np.zeros((len(test_data), n_splits))\n",
    "\n",
    "    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n",
    "        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        train_sii_mapped_train, train_sii_mapped_test = train_sii_mapped[train_idx], train_sii_mapped[test_idx]\n",
    "        \n",
    "        model = clone(model_class)\n",
    "        model.fit(X_train, train_sii_mapped_train)\n",
    "\n",
    "        y_train_pred = model.predict(X_train)\n",
    "        y_val_pred = model.predict(X_val)\n",
    "\n",
    "        oof_non_rounded[test_idx] = y_val_pred\n",
    "        y_val_pred_rounded = y_val_pred.astype(int)\n",
    "        oof_rounded[test_idx] = y_val_pred_rounded\n",
    "\n",
    "        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.astype(int))\n",
    "        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n",
    "\n",
    "        train_S.append(train_kappa)\n",
    "        test_S.append(val_kappa)\n",
    "        \n",
    "        test_preds[:, fold] = model.predict(test_data)\n",
    "        \n",
    "        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n",
    "        clear_output(wait=True)\n",
    "\n",
    "    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n",
    "    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n",
    "\n",
    "    KappaOPtimizer = minimize(evaluate_predictions,\n",
    "                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n",
    "                              method='Nelder-Mead')\n",
    "    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n",
    "    \n",
    "    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n",
    "    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n",
    "\n",
    "    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n",
    "\n",
    "    tpm = test_preds.mean(axis=1)\n",
    "    tp_rounded = threshold_Rounder(tpm, KappaOPtimizer.x)\n",
    "\n",
    "    return tp_rounded\n",
    "\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Imputation step: Filling missing values with the median\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "# Tuning\n",
    "X = train.drop(['sii'], axis=1)\n",
    "y = train_sii_mapped\n",
    "tuned_models = {\n",
    "    'lgb': tune_model(\n",
    "        LGBMRegressor(random_state=SEED), \n",
    "        {\n",
    "            'regressor__n_estimators': [500],  # Fewer values for trees\n",
    "            'regressor__learning_rate': [0.1],  # Common smaller learning rates\n",
    "            'regressor__max_depth': [7],  # Focus on simpler models\n",
    "            'regressor__num_leaves': [31],  # Smaller complexity\n",
    "            'regressor__lambda_l1': [1.0],  # Reduce L1 regularization range\n",
    "            'regressor__lambda_l2': [0.0],  # Reduce L2 regularization range\n",
    "        }, \n",
    "        X, y\n",
    "    ),\n",
    "    'xgb': tune_model(\n",
    "        XGBRegressor(random_state=SEED), \n",
    "        {\n",
    "            'regressor__n_estimators': [500],  # Same reduced range for consistency\n",
    "            'regressor__learning_rate': [0.05],\n",
    "            'regressor__max_depth': [6],\n",
    "            'regressor__subsample': [0.8],  # Limited to effective ranges\n",
    "            'regressor__colsample_bytree': [1.0],\n",
    "        }, \n",
    "        X, y\n",
    "    ),\n",
    "    'cat': tune_model(\n",
    "        CatBoostRegressor(random_state=SEED, silent=True), \n",
    "        {\n",
    "            'regressor__iterations': [500],  # Fewer iterations\n",
    "            'regressor__learning_rate': [0.1],\n",
    "            'regressor__depth': [8],\n",
    "            'regressor__l2_leaf_reg': [1],  # Simplified L2 range\n",
    "        }, \n",
    "        X, y\n",
    "    ),\n",
    "}\n",
    "\n",
    "\n",
    "# Ensemble with tuned models\n",
    "ensemble = VotingRegressor(estimators=[\n",
    "    ('lgb', tuned_models['lgb']),\n",
    "    ('xgb', tuned_models['xgb']),\n",
    "    ('cat', tuned_models['cat']),\n",
    "])\n",
    "\n",
    "\n",
    "# Train and predict\n",
    "predictions = TrainML(ensemble, test)\n",
    "\n",
    "# Save predictions\n",
    "# sample_submission = pd.read_csv('sample_submission.csv')\n",
    "sample['sii'] = predictions\n",
    "sample.to_csv('submission.csv', index=False)"
   ],
   "id": "92a41cb72ee93fdc",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Folds: 100%|██████████| 5/5 [00:14<00:00,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Train QWK --> 0.8800\n",
      "Mean Validation QWK ---> 0.4993\n",
      "----> || Optimized QWK SCORE :: \u001B[36m\u001B[1m 0.546\u001B[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-27T00:07:16.794661Z",
     "start_time": "2024-11-27T00:07:16.662479Z"
    }
   },
   "cell_type": "code",
   "source": "sample",
   "id": "c5cd7085203acdc6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          id  sii\n",
       "0   00008ff9    0\n",
       "1   000fd460    0\n",
       "2   00105258    1\n",
       "3   00115b9f    0\n",
       "4   0016bb22    1\n",
       "5   001f3379    1\n",
       "6   0038ba98    1\n",
       "7   0068a485    0\n",
       "8   0069fbed    1\n",
       "9   0083e397    1\n",
       "10  0087dd65    1\n",
       "11  00abe655    1\n",
       "12  00ae59c9    1\n",
       "13  00af6387    1\n",
       "14  00bd4359    1\n",
       "15  00c0cd71    1\n",
       "16  00d56d4b    0\n",
       "17  00d9913d    1\n",
       "18  00e6167c    0\n",
       "19  00ebc35d    1"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sii</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00008ff9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000fd460</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00105258</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00115b9f</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0016bb22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>001f3379</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0038ba98</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0068a485</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0069fbed</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0083e397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0087dd65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>00abe655</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>00ae59c9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>00af6387</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00bd4359</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>00c0cd71</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>00d56d4b</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>00d9913d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>00e6167c</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>00ebc35d</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "73b667330e37763a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
