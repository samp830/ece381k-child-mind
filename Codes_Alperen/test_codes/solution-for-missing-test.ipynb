{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 81933,
     "databundleVersionId": 9643020,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30786,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": false
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1259.762308,
   "end_time": "2024-10-16T01:29:25.368389",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-10-16T01:08:25.606081",
   "version": "2.6.0"
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": "# **Trying out a new way of loading time series data**\n\n\n\n",
   "metadata": {
    "papermill": {
     "duration": 0.023757,
     "end_time": "2024-10-16T01:08:28.971241",
     "exception": false,
     "start_time": "2024-10-16T01:08:28.947484",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "markdown",
   "source": "# The score might fluctuate from 0.483 to 0.487 due to some randomness ",
   "metadata": {
    "papermill": {
     "duration": 0.025411,
     "end_time": "2024-10-16T01:08:29.020263",
     "exception": false,
     "start_time": "2024-10-16T01:08:28.994852",
     "status": "completed"
    },
    "tags": []
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import kurtosis\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.base import clone\n",
    "from copy import deepcopy\n",
    "import optuna\n",
    "from scipy.optimize import minimize\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from colorama import Fore, Style\n",
    "\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "from sklearn.model_selection import *\n",
    "from sklearn.metrics import *\n",
    "\n",
    "SEED = 42\n",
    "n_splits = 5\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "from sklearn.base import clone\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from scipy.optimize import minimize\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "\n",
    "from colorama import Fore, Style\n",
    "from IPython.display import clear_output\n",
    "import warnings\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "\n",
    "def process_file(filename, dirname):\n",
    "    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n",
    "    df.drop('step', axis=1, inplace=True)\n",
    "\n",
    "    # Convert 'time_of_day' to datetime\n",
    "    df['time_of_day'] = pd.to_datetime(df['time_of_day'])\n",
    "    df.set_index('time_of_day', inplace=True)\n",
    "\n",
    "    # Daily aggregation\n",
    "    daily_features = df.resample('D').agg({\n",
    "        'X': ['mean', 'std', 'max', 'min', 'median', 'skew', lambda x: kurtosis(x, nan_policy='omit')],\n",
    "        'Y': ['mean', 'std', 'max', 'min', 'median', 'skew', lambda x: kurtosis(x, nan_policy='omit')],\n",
    "        'Z': ['mean', 'std', 'max', 'min', 'median', 'skew', lambda x: kurtosis(x, nan_policy='omit')],\n",
    "        'enmo': ['mean', 'std', 'max', 'min', 'median', 'skew', lambda x: kurtosis(x, nan_policy='omit')],\n",
    "        'light': ['mean', 'std'],\n",
    "        'battery_voltage': ['mean', 'std'],\n",
    "        'non-wear_flag': 'sum'\n",
    "    })\n",
    "\n",
    "    # Flatten the MultiIndex columns\n",
    "    daily_features.columns = ['_'.join(col).strip() for col in daily_features.columns.values]\n",
    "    \n",
    "    # Add additional features\n",
    "    daily_features['activity_count'] = (df['enmo'] > 0.1).resample('D').sum()  # Adjust threshold as needed\n",
    "    daily_features['days_active'] = (df['non-wear_flag'] == 0).resample('D').sum()  # Count active days\n",
    "    \n",
    "    # Rate of change features\n",
    "    daily_features['enmo_change'] = daily_features['enmo_mean'].diff()\n",
    "    \n",
    "    # Rolling statistics\n",
    "    rolling_windows = [5, 10, 15]  # Days\n",
    "    for window in rolling_windows:\n",
    "        daily_features[f'enmo_rolling_mean_{window}'] = daily_features['enmo_mean'].rolling(window=window).mean()\n",
    "        daily_features[f'enmo_rolling_std_{window}'] = daily_features['enmo_std'].rolling(window=window).std()\n",
    "\n",
    "    # Frequency domain features using FFT\n",
    "    for axis in ['X', 'Y', 'Z', 'enmo']:\n",
    "        freq_features = np.fft.fft(df[axis])\n",
    "        daily_features[f'{axis}_dominant_freq'] = np.abs(freq_features).argmax()\n",
    "\n",
    "    # Reset index to retain 'id'\n",
    "    daily_features.reset_index(inplace=True)\n",
    "    \n",
    "    # Extract child ID from filename\n",
    "    child_id = filename.split('=')[1]\n",
    "    daily_features['id'] = child_id\n",
    "\n",
    "    return daily_features\n",
    "\n",
    "\n",
    "\n",
    "def load_time_series(dirname) -> pd.DataFrame:\n",
    "    ids = os.listdir(dirname)\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n",
    "    \n",
    "    # Concatenate all results into a single DataFrame\n",
    "    features_df = pd.concat(results, ignore_index=True)\n",
    "\n",
    "    return features_df\n",
    "\n",
    "# Build the autoencoder model\n",
    "def build_autoencoder(input_dim, encoding_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    \n",
    "    # Encoder: compressing the input\n",
    "    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n",
    "    \n",
    "    # Decoder: reconstructing the input\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n",
    "    \n",
    "    # Autoencoder model\n",
    "    autoencoder = Model(inputs=input_layer, outputs=decoded)\n",
    "    \n",
    "    # Encoder model (for getting the compressed representation)\n",
    "    encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "    \n",
    "    autoencoder.compile(optimizer=Adam(), loss='mse')\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "# Function to perform dimensionality reduction using an autoencoder\n",
    "def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Perform dimensionality reduction using an Autoencoder.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame with numerical features.\n",
    "    encoding_dim (int): The dimension of the encoded space.\n",
    "    epochs (int): Number of epochs to train the autoencoder.\n",
    "    batch_size (int): Size of the batches for training.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame containing the reduced-dimensional representation (encoding).\n",
    "    \"\"\"\n",
    "    \n",
    "    # Step 1: Standardize the data\n",
    "    scaler = StandardScaler()\n",
    "    df_scaled = scaler.fit_transform(df)\n",
    "    \n",
    "    # Step 2: Build the autoencoder\n",
    "    input_dim = df_scaled.shape[1]\n",
    "    autoencoder, encoder = build_autoencoder(input_dim, encoding_dim)\n",
    "    \n",
    "    # Step 3: Train the autoencoder\n",
    "    autoencoder.fit(df_scaled, df_scaled, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=1)\n",
    "    \n",
    "    # Step 4: Get the encoded (reduced) representation\n",
    "    encoded_data = encoder.predict(df_scaled)\n",
    "    \n",
    "    # Step 5: Create a DataFrame for the encoded features\n",
    "    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i+1}' for i in range(encoded_data.shape[1])])\n",
    "    \n",
    "    return df_encoded\n",
    "\n",
    "# Load the data\n",
    "train = pd.read_csv('/Users/ad53533/Desktop/Applied ML/Project/train.csv')\n",
    "test = pd.read_csv('/Users/ad53533/Desktop/Applied ML/Project/test.csv')\n",
    "sample = pd.read_csv('/Users/ad53533/Desktop/Applied ML/Project/sample_submission.csv')\n",
    "\n",
    "train_ts = load_time_series(\"/Users/ad53533/Desktop/Applied ML/Project/series_train.parquet\")\n",
    "test_ts = load_time_series(\"/Users/ad53533/Desktop/Applied ML/Project/series_test.parquet\")"
   ],
   "metadata": {
    "papermill": {
     "duration": 529.031327,
     "end_time": "2024-10-16T01:17:28.824624",
     "exception": false,
     "start_time": "2024-10-16T01:08:39.793297",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-10-18T04:11:10.463457Z",
     "iopub.execute_input": "2024-10-18T04:11:10.463900Z",
     "iopub.status.idle": "2024-10-18T04:17:56.440187Z",
     "shell.execute_reply.started": "2024-10-18T04:11:10.463849Z",
     "shell.execute_reply": "2024-10-18T04:17:56.438031Z"
    },
    "trusted": true,
    "ExecuteTime": {
     "end_time": "2024-11-13T23:55:04.836306Z",
     "start_time": "2024-11-13T23:54:39.185205Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 157/997 [00:23<02:07,  6.60it/s]\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: '/Users/ad53533/Desktop/Applied ML/Project/series_train.parquet/.DS_Store/part-0.parquet'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNotADirectoryError\u001B[0m                        Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 189\u001B[0m\n\u001B[1;32m    186\u001B[0m test \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/Users/ad53533/Desktop/Applied ML/Project/test.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m    187\u001B[0m sample \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_csv(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/Users/ad53533/Desktop/Applied ML/Project/sample_submission.csv\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 189\u001B[0m train_ts \u001B[38;5;241m=\u001B[39m load_time_series(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Users/ad53533/Desktop/Applied ML/Project/series_train.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    190\u001B[0m test_ts \u001B[38;5;241m=\u001B[39m load_time_series(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/Users/ad53533/Desktop/Applied ML/Project/series_test.parquet\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[2], line 123\u001B[0m, in \u001B[0;36mload_time_series\u001B[0;34m(dirname)\u001B[0m\n\u001B[1;32m    120\u001B[0m ids \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mlistdir(dirname)\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ThreadPoolExecutor() \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[0;32m--> 123\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(tqdm(executor\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m fname: process_file(fname, dirname), ids), total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(ids)))\n\u001B[1;32m    125\u001B[0m \u001B[38;5;66;03m# Concatenate all results into a single DataFrame\u001B[39;00m\n\u001B[1;32m    126\u001B[0m features_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(results, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/tqdm/std.py:1181\u001B[0m, in \u001B[0;36mtqdm.__iter__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1178\u001B[0m time \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_time\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m-> 1181\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m obj \u001B[38;5;129;01min\u001B[39;00m iterable:\n\u001B[1;32m   1182\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m obj\n\u001B[1;32m   1183\u001B[0m         \u001B[38;5;66;03m# Update and possibly print the progressbar.\u001B[39;00m\n\u001B[1;32m   1184\u001B[0m         \u001B[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:619\u001B[0m, in \u001B[0;36mExecutor.map.<locals>.result_iterator\u001B[0;34m()\u001B[0m\n\u001B[1;32m    616\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m fs:\n\u001B[1;32m    617\u001B[0m     \u001B[38;5;66;03m# Careful not to keep a reference to the popped future\u001B[39;00m\n\u001B[1;32m    618\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m timeout \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m--> 619\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m _result_or_cancel(fs\u001B[38;5;241m.\u001B[39mpop())\n\u001B[1;32m    620\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    621\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m _result_or_cancel(fs\u001B[38;5;241m.\u001B[39mpop(), end_time \u001B[38;5;241m-\u001B[39m time\u001B[38;5;241m.\u001B[39mmonotonic())\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:317\u001B[0m, in \u001B[0;36m_result_or_cancel\u001B[0;34m(***failed resolving arguments***)\u001B[0m\n\u001B[1;32m    315\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    316\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 317\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m fut\u001B[38;5;241m.\u001B[39mresult(timeout)\n\u001B[1;32m    318\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    319\u001B[0m         fut\u001B[38;5;241m.\u001B[39mcancel()\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:449\u001B[0m, in \u001B[0;36mFuture.result\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    447\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m CancelledError()\n\u001B[1;32m    448\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;241m==\u001B[39m FINISHED:\n\u001B[0;32m--> 449\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__get_result()\n\u001B[1;32m    451\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_condition\u001B[38;5;241m.\u001B[39mwait(timeout)\n\u001B[1;32m    453\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_state \u001B[38;5;129;01min\u001B[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/_base.py:401\u001B[0m, in \u001B[0;36mFuture.__get_result\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    399\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception:\n\u001B[1;32m    400\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 401\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_exception\n\u001B[1;32m    402\u001B[0m     \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m    403\u001B[0m         \u001B[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001B[39;00m\n\u001B[1;32m    404\u001B[0m         \u001B[38;5;28mself\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/concurrent/futures/thread.py:58\u001B[0m, in \u001B[0;36m_WorkItem.run\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     55\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 58\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mkwargs)\n\u001B[1;32m     59\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m exc:\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfuture\u001B[38;5;241m.\u001B[39mset_exception(exc)\n",
      "Cell \u001B[0;32mIn[2], line 123\u001B[0m, in \u001B[0;36mload_time_series.<locals>.<lambda>\u001B[0;34m(fname)\u001B[0m\n\u001B[1;32m    120\u001B[0m ids \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mlistdir(dirname)\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m ThreadPoolExecutor() \u001B[38;5;28;01mas\u001B[39;00m executor:\n\u001B[0;32m--> 123\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlist\u001B[39m(tqdm(executor\u001B[38;5;241m.\u001B[39mmap(\u001B[38;5;28;01mlambda\u001B[39;00m fname: process_file(fname, dirname), ids), total\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mlen\u001B[39m(ids)))\n\u001B[1;32m    125\u001B[0m \u001B[38;5;66;03m# Concatenate all results into a single DataFrame\u001B[39;00m\n\u001B[1;32m    126\u001B[0m features_df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mconcat(results, ignore_index\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "Cell \u001B[0;32mIn[2], line 69\u001B[0m, in \u001B[0;36mprocess_file\u001B[0;34m(filename, dirname)\u001B[0m\n\u001B[1;32m     68\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mprocess_file\u001B[39m(filename, dirname):\n\u001B[0;32m---> 69\u001B[0m     df \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mread_parquet(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(dirname, filename, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mpart-0.parquet\u001B[39m\u001B[38;5;124m'\u001B[39m))\n\u001B[1;32m     70\u001B[0m     df\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mstep\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;66;03m# Convert 'time_of_day' to datetime\u001B[39;00m\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py:667\u001B[0m, in \u001B[0;36mread_parquet\u001B[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001B[0m\n\u001B[1;32m    664\u001B[0m     use_nullable_dtypes \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m    665\u001B[0m check_dtype_backend(dtype_backend)\n\u001B[0;32m--> 667\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m impl\u001B[38;5;241m.\u001B[39mread(\n\u001B[1;32m    668\u001B[0m     path,\n\u001B[1;32m    669\u001B[0m     columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[1;32m    670\u001B[0m     filters\u001B[38;5;241m=\u001B[39mfilters,\n\u001B[1;32m    671\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[1;32m    672\u001B[0m     use_nullable_dtypes\u001B[38;5;241m=\u001B[39muse_nullable_dtypes,\n\u001B[1;32m    673\u001B[0m     dtype_backend\u001B[38;5;241m=\u001B[39mdtype_backend,\n\u001B[1;32m    674\u001B[0m     filesystem\u001B[38;5;241m=\u001B[39mfilesystem,\n\u001B[1;32m    675\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    676\u001B[0m )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py:267\u001B[0m, in \u001B[0;36mPyArrowImpl.read\u001B[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001B[0m\n\u001B[1;32m    264\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m manager \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    265\u001B[0m     to_pandas_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msplit_blocks\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m path_or_handle, handles, filesystem \u001B[38;5;241m=\u001B[39m _get_path_or_handle(\n\u001B[1;32m    268\u001B[0m     path,\n\u001B[1;32m    269\u001B[0m     filesystem,\n\u001B[1;32m    270\u001B[0m     storage_options\u001B[38;5;241m=\u001B[39mstorage_options,\n\u001B[1;32m    271\u001B[0m     mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    272\u001B[0m )\n\u001B[1;32m    273\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    274\u001B[0m     pa_table \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapi\u001B[38;5;241m.\u001B[39mparquet\u001B[38;5;241m.\u001B[39mread_table(\n\u001B[1;32m    275\u001B[0m         path_or_handle,\n\u001B[1;32m    276\u001B[0m         columns\u001B[38;5;241m=\u001B[39mcolumns,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    279\u001B[0m         \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m    280\u001B[0m     )\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/parquet.py:140\u001B[0m, in \u001B[0;36m_get_path_or_handle\u001B[0;34m(path, fs, storage_options, mode, is_dir)\u001B[0m\n\u001B[1;32m    130\u001B[0m handles \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    132\u001B[0m     \u001B[38;5;129;01mnot\u001B[39;00m fs\n\u001B[1;32m    133\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_dir\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    138\u001B[0m     \u001B[38;5;66;03m# fsspec resources can also point to directories\u001B[39;00m\n\u001B[1;32m    139\u001B[0m     \u001B[38;5;66;03m# this branch is used for example when reading from non-fsspec URLs\u001B[39;00m\n\u001B[0;32m--> 140\u001B[0m     handles \u001B[38;5;241m=\u001B[39m get_handle(\n\u001B[1;32m    141\u001B[0m         path_or_handle, mode, is_text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m, storage_options\u001B[38;5;241m=\u001B[39mstorage_options\n\u001B[1;32m    142\u001B[0m     )\n\u001B[1;32m    143\u001B[0m     fs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    144\u001B[0m     path_or_handle \u001B[38;5;241m=\u001B[39m handles\u001B[38;5;241m.\u001B[39mhandle\n",
      "File \u001B[0;32m/opt/anaconda3/lib/python3.12/site-packages/pandas/io/common.py:882\u001B[0m, in \u001B[0;36mget_handle\u001B[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001B[0m\n\u001B[1;32m    873\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(\n\u001B[1;32m    874\u001B[0m             handle,\n\u001B[1;32m    875\u001B[0m             ioargs\u001B[38;5;241m.\u001B[39mmode,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    878\u001B[0m             newline\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    879\u001B[0m         )\n\u001B[1;32m    880\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    881\u001B[0m         \u001B[38;5;66;03m# Binary mode\u001B[39;00m\n\u001B[0;32m--> 882\u001B[0m         handle \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mopen\u001B[39m(handle, ioargs\u001B[38;5;241m.\u001B[39mmode)\n\u001B[1;32m    883\u001B[0m     handles\u001B[38;5;241m.\u001B[39mappend(handle)\n\u001B[1;32m    885\u001B[0m \u001B[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001B[39;00m\n",
      "\u001B[0;31mNotADirectoryError\u001B[0m: [Errno 20] Not a directory: '/Users/ad53533/Desktop/Applied ML/Project/series_train.parquet/.DS_Store/part-0.parquet'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": "# Columns to drop\ncolumns_to_drop = [\n    'enmo_change',\n    'enmo_rolling_mean_5',\n    'enmo_rolling_std_5',\n    'enmo_rolling_mean_10',\n    'enmo_rolling_std_10',\n    'enmo_rolling_mean_15',\n    'enmo_rolling_std_15'\n]\n\n# Drop the columns\ntrain_ts.drop(columns=columns_to_drop, inplace=True)\n\n# Drop the columns\ntest_ts.drop(columns=columns_to_drop, inplace=True)\n",
   "metadata": {
    "papermill": {
     "duration": 0.087567,
     "end_time": "2024-10-16T01:17:28.982636",
     "exception": false,
     "start_time": "2024-10-16T01:17:28.895069",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-10-18T04:18:59.902628Z",
     "iopub.execute_input": "2024-10-18T04:18:59.903058Z",
     "iopub.status.idle": "2024-10-18T04:19:02.162048Z",
     "shell.execute_reply.started": "2024-10-18T04:18:59.903019Z",
     "shell.execute_reply": "2024-10-18T04:19:02.159690Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[3], line 13\u001B[0m\n\u001B[1;32m      2\u001B[0m columns_to_drop \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124menmo_change\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124menmo_rolling_mean_5\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124menmo_rolling_std_15\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m     10\u001B[0m ]\n\u001B[1;32m     12\u001B[0m \u001B[38;5;66;03m# Drop the columns\u001B[39;00m\n\u001B[0;32m---> 13\u001B[0m \u001B[43mtrain_ts\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns_to_drop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;66;03m# Drop the columns\u001B[39;00m\n\u001B[1;32m     16\u001B[0m test_ts\u001B[38;5;241m.\u001B[39mdrop(columns\u001B[38;5;241m=\u001B[39mcolumns_to_drop, inplace\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/frame.py:5581\u001B[0m, in \u001B[0;36mDataFrame.drop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   5433\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdrop\u001B[39m(\n\u001B[1;32m   5434\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   5435\u001B[0m     labels: IndexLabel \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5442\u001B[0m     errors: IgnoreRaise \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m   5443\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   5444\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   5445\u001B[0m \u001B[38;5;124;03m    Drop specified labels from rows or columns.\u001B[39;00m\n\u001B[1;32m   5446\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   5579\u001B[0m \u001B[38;5;124;03m            weight  1.0     0.8\u001B[39;00m\n\u001B[1;32m   5580\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 5581\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   5582\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5583\u001B[0m \u001B[43m        \u001B[49m\u001B[43maxis\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5584\u001B[0m \u001B[43m        \u001B[49m\u001B[43mindex\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5585\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcolumns\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcolumns\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5586\u001B[0m \u001B[43m        \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5587\u001B[0m \u001B[43m        \u001B[49m\u001B[43minplace\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minplace\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5588\u001B[0m \u001B[43m        \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   5589\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:4788\u001B[0m, in \u001B[0;36mNDFrame.drop\u001B[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001B[0m\n\u001B[1;32m   4786\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m axis, labels \u001B[38;5;129;01min\u001B[39;00m axes\u001B[38;5;241m.\u001B[39mitems():\n\u001B[1;32m   4787\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 4788\u001B[0m         obj \u001B[38;5;241m=\u001B[39m \u001B[43mobj\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_drop_axis\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maxis\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlevel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlevel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4790\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m inplace:\n\u001B[1;32m   4791\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_inplace(obj)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py:4830\u001B[0m, in \u001B[0;36mNDFrame._drop_axis\u001B[0;34m(self, labels, axis, level, errors, only_slice)\u001B[0m\n\u001B[1;32m   4828\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mdrop(labels, level\u001B[38;5;241m=\u001B[39mlevel, errors\u001B[38;5;241m=\u001B[39merrors)\n\u001B[1;32m   4829\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 4830\u001B[0m         new_axis \u001B[38;5;241m=\u001B[39m \u001B[43maxis\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdrop\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43merrors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43merrors\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   4831\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m axis\u001B[38;5;241m.\u001B[39mget_indexer(new_axis)\n\u001B[1;32m   4833\u001B[0m \u001B[38;5;66;03m# Case for non-unique axis\u001B[39;00m\n\u001B[1;32m   4834\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/indexes/base.py:7070\u001B[0m, in \u001B[0;36mIndex.drop\u001B[0;34m(self, labels, errors)\u001B[0m\n\u001B[1;32m   7068\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m mask\u001B[38;5;241m.\u001B[39many():\n\u001B[1;32m   7069\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m errors \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mignore\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m-> 7070\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mKeyError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlabels[mask]\u001B[38;5;241m.\u001B[39mtolist()\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m not found in axis\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   7071\u001B[0m     indexer \u001B[38;5;241m=\u001B[39m indexer[\u001B[38;5;241m~\u001B[39mmask]\n\u001B[1;32m   7072\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdelete(indexer)\n",
      "\u001B[0;31mKeyError\u001B[0m: \"['enmo_change', 'enmo_rolling_mean_5', 'enmo_rolling_std_5', 'enmo_rolling_mean_10', 'enmo_rolling_std_10', 'enmo_rolling_mean_15', 'enmo_rolling_std_15'] not found in axis\""
     ],
     "ename": "KeyError",
     "evalue": "\"['enmo_change', 'enmo_rolling_mean_5', 'enmo_rolling_std_5', 'enmo_rolling_mean_10', 'enmo_rolling_std_10', 'enmo_rolling_mean_15', 'enmo_rolling_std_15'] not found in axis\"",
     "output_type": "error"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "source": "train_ts",
   "metadata": {
    "papermill": {
     "duration": 0.13241,
     "end_time": "2024-10-16T01:17:29.183102",
     "exception": false,
     "start_time": "2024-10-16T01:17:29.050692",
     "status": "completed"
    },
    "tags": [],
    "execution": {
     "iopub.status.busy": "2024-10-18T04:19:02.163752Z",
     "iopub.status.idle": "2024-10-18T04:19:02.164522Z",
     "shell.execute_reply.started": "2024-10-18T04:19:02.164049Z",
     "shell.execute_reply": "2024-10-18T04:19:02.164082Z"
    },
    "trusted": true
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Drop 'id' column for training\ndf_train = train_ts.drop('id', axis=1)\ndf_test = test_ts.drop('id', axis=1)",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:17:29.327151Z",
     "iopub.status.busy": "2024-10-16T01:17:29.326699Z",
     "iopub.status.idle": "2024-10-16T01:17:29.33619Z",
     "shell.execute_reply": "2024-10-16T01:17:29.335009Z"
    },
    "papermill": {
     "duration": 0.083267,
     "end_time": "2024-10-16T01:17:29.338866",
     "exception": false,
     "start_time": "2024-10-16T01:17:29.255599",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df_train.dtypes",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:17:29.477849Z",
     "iopub.status.busy": "2024-10-16T01:17:29.477338Z",
     "iopub.status.idle": "2024-10-16T01:17:29.489096Z",
     "shell.execute_reply": "2024-10-16T01:17:29.487789Z"
    },
    "papermill": {
     "duration": 0.08524,
     "end_time": "2024-10-16T01:17:29.491747",
     "exception": false,
     "start_time": "2024-10-16T01:17:29.406507",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "df_train",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:17:29.632954Z",
     "iopub.status.busy": "2024-10-16T01:17:29.631801Z",
     "iopub.status.idle": "2024-10-16T01:17:29.68504Z",
     "shell.execute_reply": "2024-10-16T01:17:29.683756Z"
    },
    "papermill": {
     "duration": 0.125655,
     "end_time": "2024-10-16T01:17:29.687851",
     "exception": false,
     "start_time": "2024-10-16T01:17:29.562196",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\n\ndef build_autoencoder(input_dim, encoding_dim):\n    input_layer = Input(shape=(input_dim,))\n    \n    # Encoder: compressing the input\n    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n    \n    # Decoder: reconstructing the input\n    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n    \n    # Autoencoder model\n    autoencoder = Model(inputs=input_layer, outputs=decoded)\n    \n    # Encoder model (for getting the compressed representation)\n    encoder = Model(inputs=input_layer, outputs=encoded)\n    \n    autoencoder.compile(optimizer=Adam(), loss='mse')\n    \n    return autoencoder, encoder\n\ndef perform_autoencoder(df, encoding_dim=50, epochs=200, batch_size=32):\n    scaler = StandardScaler()\n    \n    # Exclude non-numeric columns\n    df_numeric = df.select_dtypes(include=[np.number])\n    \n    # Check for and handle NaN values\n    if df_numeric.isnull().values.any():\n        df_numeric.fillna(df_numeric.mean(), inplace=True)\n\n    if np.isinf(df_numeric).values.any():\n        df_numeric.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df_numeric.fillna(df_numeric.mean(), inplace=True)\n\n    df_scaled = scaler.fit_transform(df_numeric)\n\n    # Check for NaN values after scaling\n    if np.isnan(df_scaled).any() or np.isinf(df_scaled).any():\n        print(\"NaN or infinite values detected in the scaled data!\")\n        return None  # or handle as needed\n\n    input_dim = df_scaled.shape[1]\n    autoencoder, encoder = build_autoencoder(input_dim, encoding_dim)\n    \n    # Use a smaller learning rate for stability\n    autoencoder.compile(optimizer=Adam(learning_rate=0.0001), loss='mse')\n    \n    # Set up early stopping\n    early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n\n    # Train the autoencoder with early stopping\n    autoencoder.fit(df_scaled, df_scaled, \n                    epochs=epochs, \n                    batch_size=batch_size, \n                    shuffle=True, \n                    verbose=1, \n                    callbacks=[early_stopping])\n\n    encoded_data = encoder.predict(df_scaled)\n    \n    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i+1}' for i in range(encoded_data.shape[1])])\n    \n    return df_encoded\n",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:17:29.831277Z",
     "iopub.status.busy": "2024-10-16T01:17:29.830832Z",
     "iopub.status.idle": "2024-10-16T01:17:46.157065Z",
     "shell.execute_reply": "2024-10-16T01:17:46.155756Z"
    },
    "papermill": {
     "duration": 16.400711,
     "end_time": "2024-10-16T01:17:46.160059",
     "exception": false,
     "start_time": "2024-10-16T01:17:29.759348",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# # Perform autoencoder dimensionality reduction\n# train_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\n# test_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n\ntrain_ts_encoded = perform_autoencoder(df_train, encoding_dim=64, epochs=256, batch_size=32)\ntest_ts_encoded = perform_autoencoder(df_test, encoding_dim=64,epochs=256, batch_size=32)\n",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:17:46.4021Z",
     "iopub.status.busy": "2024-10-16T01:17:46.401233Z",
     "iopub.status.idle": "2024-10-16T01:18:21.831539Z",
     "shell.execute_reply": "2024-10-16T01:18:21.830379Z"
    },
    "papermill": {
     "duration": 35.604464,
     "end_time": "2024-10-16T01:18:21.834548",
     "exception": false,
     "start_time": "2024-10-16T01:17:46.230084",
     "status": "completed"
    },
    "tags": [],
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train_ts_encoded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:22.295655Z",
     "iopub.status.busy": "2024-10-16T01:18:22.295202Z",
     "iopub.status.idle": "2024-10-16T01:18:22.372643Z",
     "shell.execute_reply": "2024-10-16T01:18:22.371511Z"
    },
    "papermill": {
     "duration": 0.308849,
     "end_time": "2024-10-16T01:18:22.375663",
     "exception": false,
     "start_time": "2024-10-16T01:18:22.066814",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "test_ts_encoded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:22.863303Z",
     "iopub.status.busy": "2024-10-16T01:18:22.862503Z",
     "iopub.status.idle": "2024-10-16T01:18:22.926257Z",
     "shell.execute_reply": "2024-10-16T01:18:22.924745Z"
    },
    "papermill": {
     "duration": 0.31888,
     "end_time": "2024-10-16T01:18:22.928964",
     "exception": false,
     "start_time": "2024-10-16T01:18:22.610084",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train_ts_encoded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:23.401031Z",
     "iopub.status.busy": "2024-10-16T01:18:23.400533Z",
     "iopub.status.idle": "2024-10-16T01:18:23.481968Z",
     "shell.execute_reply": "2024-10-16T01:18:23.480754Z"
    },
    "papermill": {
     "duration": 0.31544,
     "end_time": "2024-10-16T01:18:23.484552",
     "exception": false,
     "start_time": "2024-10-16T01:18:23.169112",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "time_series_cols = train_ts_encoded.columns.tolist()\n#time_series_cols.remove(\"id\")",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:23.952914Z",
     "iopub.status.busy": "2024-10-16T01:18:23.952207Z",
     "iopub.status.idle": "2024-10-16T01:18:23.958494Z",
     "shell.execute_reply": "2024-10-16T01:18:23.957121Z"
    },
    "papermill": {
     "duration": 0.244567,
     "end_time": "2024-10-16T01:18:23.961403",
     "exception": false,
     "start_time": "2024-10-16T01:18:23.716836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train_ts_encoded[\"id\"]=train_ts[\"id\"]\n\ntest_ts_encoded['id']=test_ts[\"id\"]\n#test_ts_pca[\"id\"]=test_ts[\"id\"]",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:24.42775Z",
     "iopub.status.busy": "2024-10-16T01:18:24.426477Z",
     "iopub.status.idle": "2024-10-16T01:18:24.433767Z",
     "shell.execute_reply": "2024-10-16T01:18:24.432655Z"
    },
    "papermill": {
     "duration": 0.241313,
     "end_time": "2024-10-16T01:18:24.436402",
     "exception": false,
     "start_time": "2024-10-16T01:18:24.195089",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train_ts_encoded",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:24.907076Z",
     "iopub.status.busy": "2024-10-16T01:18:24.906571Z",
     "iopub.status.idle": "2024-10-16T01:18:24.99296Z",
     "shell.execute_reply": "2024-10-16T01:18:24.991523Z"
    },
    "papermill": {
     "duration": 0.323537,
     "end_time": "2024-10-16T01:18:24.995631",
     "exception": false,
     "start_time": "2024-10-16T01:18:24.672094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#train = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n#test_ = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n#test = pd.merge(test, test_ts, how=\"inner\", on='id')",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:25.47099Z",
     "iopub.status.busy": "2024-10-16T01:18:25.470498Z",
     "iopub.status.idle": "2024-10-16T01:18:25.503677Z",
     "shell.execute_reply": "2024-10-16T01:18:25.502427Z"
    },
    "papermill": {
     "duration": 0.274048,
     "end_time": "2024-10-16T01:18:25.506521",
     "exception": false,
     "start_time": "2024-10-16T01:18:25.232473",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "test.shape",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:26.037954Z",
     "iopub.status.busy": "2024-10-16T01:18:26.036889Z",
     "iopub.status.idle": "2024-10-16T01:18:26.044951Z",
     "shell.execute_reply": "2024-10-16T01:18:26.043792Z"
    },
    "papermill": {
     "duration": 0.249054,
     "end_time": "2024-10-16T01:18:26.047596",
     "exception": false,
     "start_time": "2024-10-16T01:18:25.798542",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train.shape",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:26.519743Z",
     "iopub.status.busy": "2024-10-16T01:18:26.519149Z",
     "iopub.status.idle": "2024-10-16T01:18:26.52696Z",
     "shell.execute_reply": "2024-10-16T01:18:26.525687Z"
    },
    "papermill": {
     "duration": 0.246574,
     "end_time": "2024-10-16T01:18:26.529568",
     "exception": false,
     "start_time": "2024-10-16T01:18:26.282994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "analysis=train",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:27.006068Z",
     "iopub.status.busy": "2024-10-16T01:18:27.004913Z",
     "iopub.status.idle": "2024-10-16T01:18:27.010671Z",
     "shell.execute_reply": "2024-10-16T01:18:27.009277Z"
    },
    "papermill": {
     "duration": 0.245691,
     "end_time": "2024-10-16T01:18:27.013373",
     "exception": false,
     "start_time": "2024-10-16T01:18:26.767682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "analysis = analysis.dropna(subset='Enc_4')",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:27.487592Z",
     "iopub.status.busy": "2024-10-16T01:18:27.487134Z",
     "iopub.status.idle": "2024-10-16T01:18:27.496117Z",
     "shell.execute_reply": "2024-10-16T01:18:27.495109Z"
    },
    "papermill": {
     "duration": 0.2491,
     "end_time": "2024-10-16T01:18:27.498662",
     "exception": false,
     "start_time": "2024-10-16T01:18:27.249562",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "analysis=analysis.dropna(subset='sii')",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:27.971446Z",
     "iopub.status.busy": "2024-10-16T01:18:27.971023Z",
     "iopub.status.idle": "2024-10-16T01:18:27.978927Z",
     "shell.execute_reply": "2024-10-16T01:18:27.977646Z"
    },
    "papermill": {
     "duration": 0.245138,
     "end_time": "2024-10-16T01:18:27.981566",
     "exception": false,
     "start_time": "2024-10-16T01:18:27.736428",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "analysis = analysis.drop('id', axis=1)",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:28.454413Z",
     "iopub.status.busy": "2024-10-16T01:18:28.453964Z",
     "iopub.status.idle": "2024-10-16T01:18:28.46202Z",
     "shell.execute_reply": "2024-10-16T01:18:28.460567Z"
    },
    "papermill": {
     "duration": 0.245555,
     "end_time": "2024-10-16T01:18:28.464618",
     "exception": false,
     "start_time": "2024-10-16T01:18:28.219063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "def engineer_features(df):\n    # Create interaction features\n    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n    \n    # Create categorical features\n    #df['Age_Group'] = pd.cut(df['Basic_Demos-Age'], bins=[0, 12, 18, 25, 100], labels=['Child', 'Teen', 'Young Adult', 'Adult'])\n    #df['BMI_Category'] = pd.cut(df['Physical-BMI'], bins=[0, 18.5, 25, 30, 100], labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n    \n    return df\n\ntest = engineer_features(test)",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:28.999848Z",
     "iopub.status.busy": "2024-10-16T01:18:28.99939Z",
     "iopub.status.idle": "2024-10-16T01:18:29.009934Z",
     "shell.execute_reply": "2024-10-16T01:18:29.00876Z"
    },
    "papermill": {
     "duration": 0.248911,
     "end_time": "2024-10-16T01:18:29.012551",
     "exception": false,
     "start_time": "2024-10-16T01:18:28.76364",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "test.shape",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:29.482646Z",
     "iopub.status.busy": "2024-10-16T01:18:29.482158Z",
     "iopub.status.idle": "2024-10-16T01:18:29.49012Z",
     "shell.execute_reply": "2024-10-16T01:18:29.488757Z"
    },
    "papermill": {
     "duration": 0.246176,
     "end_time": "2024-10-16T01:18:29.492962",
     "exception": false,
     "start_time": "2024-10-16T01:18:29.246786",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train=engineer_features(train)",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:29.969829Z",
     "iopub.status.busy": "2024-10-16T01:18:29.969327Z",
     "iopub.status.idle": "2024-10-16T01:18:29.978393Z",
     "shell.execute_reply": "2024-10-16T01:18:29.977215Z"
    },
    "papermill": {
     "duration": 0.252,
     "end_time": "2024-10-16T01:18:29.981021",
     "exception": false,
     "start_time": "2024-10-16T01:18:29.729021",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train.shape",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:30.454833Z",
     "iopub.status.busy": "2024-10-16T01:18:30.454378Z",
     "iopub.status.idle": "2024-10-16T01:18:30.461971Z",
     "shell.execute_reply": "2024-10-16T01:18:30.460758Z"
    },
    "papermill": {
     "duration": 0.246652,
     "end_time": "2024-10-16T01:18:30.464766",
     "exception": false,
     "start_time": "2024-10-16T01:18:30.218114",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "#train = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:30.941708Z",
     "iopub.status.busy": "2024-10-16T01:18:30.940626Z",
     "iopub.status.idle": "2024-10-16T01:18:30.948299Z",
     "shell.execute_reply": "2024-10-16T01:18:30.946789Z"
    },
    "papermill": {
     "duration": 0.248613,
     "end_time": "2024-10-16T01:18:30.951172",
     "exception": false,
     "start_time": "2024-10-16T01:18:30.702559",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "nan_rows = train.query('sii.isna()', engine='python')",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:31.422634Z",
     "iopub.status.busy": "2024-10-16T01:18:31.421643Z",
     "iopub.status.idle": "2024-10-16T01:18:31.471067Z",
     "shell.execute_reply": "2024-10-16T01:18:31.46943Z"
    },
    "papermill": {
     "duration": 0.287418,
     "end_time": "2024-10-16T01:18:31.475065",
     "exception": false,
     "start_time": "2024-10-16T01:18:31.187647",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# train.isna().sum().sum()-train_imputed.isna().sum().sum()",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:31.943336Z",
     "iopub.status.busy": "2024-10-16T01:18:31.942896Z",
     "iopub.status.idle": "2024-10-16T01:18:31.94803Z",
     "shell.execute_reply": "2024-10-16T01:18:31.946888Z"
    },
    "papermill": {
     "duration": 0.242642,
     "end_time": "2024-10-16T01:18:31.950489",
     "exception": false,
     "start_time": "2024-10-16T01:18:31.707847",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nfrom sklearn.impute import KNNImputer\n\n# Assuming 'train' is your DataFrame\n\n# Step 1: Create the KNN imputer\nimputer = KNNImputer(n_neighbors=5)  # You can adjust the number of neighbors\n\n# Step 2: Select numeric columns and fit the imputer\nnumeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\nimputed_data = imputer.fit_transform(train[numeric_cols])\n\n# Step 3: Create a new DataFrame with the imputed values\ntrain_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n\n# Step 4: Convert the 'sii' column back to integers\ntrain_imputed['sii'] = train_imputed['sii'].round().astype(int)\n\n# If there are other columns to retain, you can merge them back\nfor col in train.columns:\n    if col not in numeric_cols:\n        train_imputed[col] = train[col]\n\n# Now, check if 'sii' has been filled and is of integer type\nprint(train_imputed['sii'].isna().sum())  # Should be 0 if all NaNs were filled\nprint(train_imputed['sii'].dtype)  # Should show 'int'\n\ntrain_imputed['sii']=train['sii']\n\ntrain=train_imputed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:32.421364Z",
     "iopub.status.busy": "2024-10-16T01:18:32.420343Z",
     "iopub.status.idle": "2024-10-16T01:18:42.016932Z",
     "shell.execute_reply": "2024-10-16T01:18:42.015432Z"
    },
    "papermill": {
     "duration": 9.834466,
     "end_time": "2024-10-16T01:18:42.019744",
     "exception": false,
     "start_time": "2024-10-16T01:18:32.185278",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%time\n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii','BMI_Age','Internet_Hours_Age','BMI_Internet_Hours']\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\n#test= test[featuresCols]\ntrain = train.dropna(subset='sii')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', 'Fitness_Endurance-Season', \n          'FGC-Season', 'BIA-Season', 'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ndef update(df):\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:42.551977Z",
     "iopub.status.busy": "2024-10-16T01:18:42.551501Z",
     "iopub.status.idle": "2024-10-16T01:18:42.576105Z",
     "shell.execute_reply": "2024-10-16T01:18:42.574636Z"
    },
    "papermill": {
     "duration": 0.323965,
     "end_time": "2024-10-16T01:18:42.578795",
     "exception": false,
     "start_time": "2024-10-16T01:18:42.25483",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "featuresCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday','BMI_Age','Internet_Hours_Age','BMI_Internet_Hours']\n\nfeaturesCols += time_series_cols\n",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:43.086002Z",
     "iopub.status.busy": "2024-10-16T01:18:43.084769Z",
     "iopub.status.idle": "2024-10-16T01:18:43.094544Z",
     "shell.execute_reply": "2024-10-16T01:18:43.093094Z"
    },
    "papermill": {
     "duration": 0.269703,
     "end_time": "2024-10-16T01:18:43.097256",
     "exception": false,
     "start_time": "2024-10-16T01:18:42.827553",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "test= test[featuresCols]",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:43.576439Z",
     "iopub.status.busy": "2024-10-16T01:18:43.575967Z",
     "iopub.status.idle": "2024-10-16T01:18:43.583468Z",
     "shell.execute_reply": "2024-10-16T01:18:43.582164Z"
    },
    "papermill": {
     "duration": 0.249834,
     "end_time": "2024-10-16T01:18:43.586147",
     "exception": false,
     "start_time": "2024-10-16T01:18:43.336313",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# #train['Age_Group'] = train['Age_Group'].cat.add_categories(['Missing'])\n\n# # Step 2: Fill missing values with 'Missing'\n# #train['Age_Group'] = train['Age_Group'].fillna('Missing')\n\n#train['BMI_Category'] = train['BMI_Category'].cat.add_categories(['Missing'])\n\n# # Step 2: Fill missing values with 'Missing'\n#train['BMI_Category'] = train['BMI_Category'].fillna('Missing')",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:44.058939Z",
     "iopub.status.busy": "2024-10-16T01:18:44.058431Z",
     "iopub.status.idle": "2024-10-16T01:18:44.063995Z",
     "shell.execute_reply": "2024-10-16T01:18:44.062779Z"
    },
    "papermill": {
     "duration": 0.244868,
     "end_time": "2024-10-16T01:18:44.066745",
     "exception": false,
     "start_time": "2024-10-16T01:18:43.821877",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# #test['Age_Group'] = test['Age_Group'].cat.add_categories(['Missing'])\n\n# # Step 2: Fill missing values with 'Missing'\n# #test['Age_Group'] = test['Age_Group'].fillna('Missing')\n\n#test['BMI_Category'] = test['BMI_Category'].cat.add_categories(['Missing'])\n\n# # Step 2: Fill missing values with 'Missing'\n#test['BMI_Category'] = test['BMI_Category'].fillna('Missing')",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:44.540169Z",
     "iopub.status.busy": "2024-10-16T01:18:44.539679Z",
     "iopub.status.idle": "2024-10-16T01:18:44.545102Z",
     "shell.execute_reply": "2024-10-16T01:18:44.543912Z"
    },
    "papermill": {
     "duration": 0.246725,
     "end_time": "2024-10-16T01:18:44.547688",
     "exception": false,
     "start_time": "2024-10-16T01:18:44.300963",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "test",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:45.024004Z",
     "iopub.status.busy": "2024-10-16T01:18:45.023494Z",
     "iopub.status.idle": "2024-10-16T01:18:45.247601Z",
     "shell.execute_reply": "2024-10-16T01:18:45.246366Z"
    },
    "papermill": {
     "duration": 0.470339,
     "end_time": "2024-10-16T01:18:45.252298",
     "exception": false,
     "start_time": "2024-10-16T01:18:44.781959",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# #train = train.drop('BMI_Category', axis=1)\n# test = test.drop('BMI_Category', axis=1)",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:45.736381Z",
     "iopub.status.busy": "2024-10-16T01:18:45.73585Z",
     "iopub.status.idle": "2024-10-16T01:18:45.740973Z",
     "shell.execute_reply": "2024-10-16T01:18:45.739754Z"
    },
    "papermill": {
     "duration": 0.250846,
     "end_time": "2024-10-16T01:18:45.743646",
     "exception": false,
     "start_time": "2024-10-16T01:18:45.4928",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train = update(train)\ntest = update(test)\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:46.287132Z",
     "iopub.status.busy": "2024-10-16T01:18:46.286625Z",
     "iopub.status.idle": "2024-10-16T01:18:46.321779Z",
     "shell.execute_reply": "2024-10-16T01:18:46.32058Z"
    },
    "papermill": {
     "duration": 0.340734,
     "end_time": "2024-10-16T01:18:46.324617",
     "exception": false,
     "start_time": "2024-10-16T01:18:45.983883",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "\"\"\"This Mapping Works Fine For me I also Check Each Values in Train and test Using Logic. There no Data Lekage.\"\"\"\n\nfor col in cat_c:\n    mapping_train = create_mapping(col, train)\n    mapping_test = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping_train).astype(int)\n    test[col] = test[col].replace(mapping_test).astype(int)\n\nprint(f'Train Shape : {train.shape} || Test Shape : {test.shape}')",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:46.807888Z",
     "iopub.status.busy": "2024-10-16T01:18:46.807372Z",
     "iopub.status.idle": "2024-10-16T01:18:46.863029Z",
     "shell.execute_reply": "2024-10-16T01:18:46.861767Z"
    },
    "papermill": {
     "duration": 0.299425,
     "end_time": "2024-10-16T01:18:46.865841",
     "exception": false,
     "start_time": "2024-10-16T01:18:46.566416",
     "status": "completed"
    },
    "tags": [],
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%time\n\ntrain.head()",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:47.348875Z",
     "iopub.status.busy": "2024-10-16T01:18:47.347436Z",
     "iopub.status.idle": "2024-10-16T01:18:47.470404Z",
     "shell.execute_reply": "2024-10-16T01:18:47.469122Z"
    },
    "papermill": {
     "duration": 0.368789,
     "end_time": "2024-10-16T01:18:47.47369",
     "exception": false,
     "start_time": "2024-10-16T01:18:47.104901",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "%%time\n\ntest.head()",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:47.962134Z",
     "iopub.status.busy": "2024-10-16T01:18:47.961664Z",
     "iopub.status.idle": "2024-10-16T01:18:48.097414Z",
     "shell.execute_reply": "2024-10-16T01:18:48.096088Z"
    },
    "papermill": {
     "duration": 0.383055,
     "end_time": "2024-10-16T01:18:48.100195",
     "exception": false,
     "start_time": "2024-10-16T01:18:47.71714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train.sii.unique()",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:48.594457Z",
     "iopub.status.busy": "2024-10-16T01:18:48.593974Z",
     "iopub.status.idle": "2024-10-16T01:18:48.602937Z",
     "shell.execute_reply": "2024-10-16T01:18:48.601767Z"
    },
    "papermill": {
     "duration": 0.261251,
     "end_time": "2024-10-16T01:18:48.605621",
     "exception": false,
     "start_time": "2024-10-16T01:18:48.34437",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# Find columns missing in df2 but present in df1\nset(train.columns) - set(test.columns)",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:49.102212Z",
     "iopub.status.busy": "2024-10-16T01:18:49.101731Z",
     "iopub.status.idle": "2024-10-16T01:18:49.110366Z",
     "shell.execute_reply": "2024-10-16T01:18:49.109079Z"
    },
    "papermill": {
     "duration": 0.260913,
     "end_time": "2024-10-16T01:18:49.113171",
     "exception": false,
     "start_time": "2024-10-16T01:18:48.852258",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "analysis=train.dropna(subset='Enc_51')",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:49.607192Z",
     "iopub.status.busy": "2024-10-16T01:18:49.606675Z",
     "iopub.status.idle": "2024-10-16T01:18:49.619033Z",
     "shell.execute_reply": "2024-10-16T01:18:49.617738Z"
    },
    "papermill": {
     "duration": 0.261936,
     "end_time": "2024-10-16T01:18:49.621953",
     "exception": false,
     "start_time": "2024-10-16T01:18:49.360017",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "missing_df=train[train['Enc_51'].isna()]",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:50.17964Z",
     "iopub.status.busy": "2024-10-16T01:18:50.179162Z",
     "iopub.status.idle": "2024-10-16T01:18:50.189124Z",
     "shell.execute_reply": "2024-10-16T01:18:50.187704Z"
    },
    "papermill": {
     "duration": 0.260913,
     "end_time": "2024-10-16T01:18:50.192051",
     "exception": false,
     "start_time": "2024-10-16T01:18:49.931138",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "missing_df",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:50.68175Z",
     "iopub.status.busy": "2024-10-16T01:18:50.68129Z",
     "iopub.status.idle": "2024-10-16T01:18:50.835394Z",
     "shell.execute_reply": "2024-10-16T01:18:50.834035Z"
    },
    "papermill": {
     "duration": 0.405512,
     "end_time": "2024-10-16T01:18:50.839957",
     "exception": false,
     "start_time": "2024-10-16T01:18:50.434445",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\n\n# Assuming analysis is your complete DataFrame (1000 data points)\n# and missing_df is your DataFrame with missing values (3000 data points)\n\n# Step 1: Store original data types\noriginal_dtypes = analysis.dtypes.to_dict()\n\n# Step 2: Split the complete data into features and target\nX_complete = analysis.drop('sii', axis=1)\ny_complete = analysis['sii']\n\n# Step 3: Identify categorical and numerical columns\ncategorical_features = [\n    'Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n    'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n    'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season'\n]\n\n# Step 3.1: Drop categorical columns for mean calculation\nnumerical_df = analysis.drop(columns=categorical_features + ['sii'])\n\n# Group by the target variable and calculate means for numerical features\nmean_impute = numerical_df.groupby(analysis['sii']).mean()\n\n# Group by the target variable and calculate modes for categorical features\nmode_impute = analysis[categorical_features].groupby(analysis['sii']).agg(\n    lambda x: x.mode()[0] if not x.mode().empty else None\n)\n\n# Step 4: Fill missing values class-wise\ndef fill_missing_values(row):\n    class_value = row['sii']\n    if pd.isnull(class_value):\n        return row  # No action if class_value is NaN\n\n    for col in row.index:\n        if pd.isnull(row[col]):\n            if col in mean_impute.columns and class_value in mean_impute.index:  # For numerical columns\n                row[col] = mean_impute.loc[class_value, col]\n            elif col in mode_impute.columns and class_value in mode_impute.index:  # For categorical columns\n                row[col] = mode_impute.loc[class_value, col]\n    return row\n\n# Apply the filling function to each row in the missing_df\ndf_missing_filled = missing_df.apply(fill_missing_values, axis=1)\n\n# Step 5: Combine datasets\nfinal_dataset = pd.concat([analysis, df_missing_filled], ignore_index=True)\n\n# Step 6: Convert categorical features to int64 safely\nfor col in categorical_features:\n    # Check for unique values and their counts\n    unique_values = final_dataset[col].unique()\n    print(f'Unique values in {col}: {unique_values}')\n\n    # Fill NaNs with a placeholder (optional)\n    final_dataset[col] = final_dataset[col].fillna('missing_value')\n\n    # Convert to category type first to manage the encoding\n    final_dataset[col] = final_dataset[col].astype('category')\n    \n    # Factorize the column\n    final_dataset[col] = pd.factorize(final_dataset[col])[0].astype('int64')\n\n# Step 7: Restore original data types for other columns\nfinal_dataset = final_dataset.astype(original_dtypes)\n\n# Now, final_dataset contains all your data with missing values filled and original data types restored\n",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:51.336404Z",
     "iopub.status.busy": "2024-10-16T01:18:51.335922Z",
     "iopub.status.idle": "2024-10-16T01:18:59.404785Z",
     "shell.execute_reply": "2024-10-16T01:18:59.403238Z"
    },
    "papermill": {
     "duration": 8.319234,
     "end_time": "2024-10-16T01:18:59.407552",
     "exception": false,
     "start_time": "2024-10-16T01:18:51.088318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "mean_impute.columns",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:18:59.904767Z",
     "iopub.status.busy": "2024-10-16T01:18:59.903543Z",
     "iopub.status.idle": "2024-10-16T01:18:59.913856Z",
     "shell.execute_reply": "2024-10-16T01:18:59.912615Z"
    },
    "papermill": {
     "duration": 0.260074,
     "end_time": "2024-10-16T01:18:59.9163",
     "exception": false,
     "start_time": "2024-10-16T01:18:59.656226",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "mode_impute.columns",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:19:00.411962Z",
     "iopub.status.busy": "2024-10-16T01:19:00.411518Z",
     "iopub.status.idle": "2024-10-16T01:19:00.419301Z",
     "shell.execute_reply": "2024-10-16T01:19:00.418097Z"
    },
    "papermill": {
     "duration": 0.2574,
     "end_time": "2024-10-16T01:19:00.422228",
     "exception": false,
     "start_time": "2024-10-16T01:19:00.164828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "train=final_dataset",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:19:00.922405Z",
     "iopub.status.busy": "2024-10-16T01:19:00.92154Z",
     "iopub.status.idle": "2024-10-16T01:19:00.927166Z",
     "shell.execute_reply": "2024-10-16T01:19:00.925919Z"
    },
    "papermill": {
     "duration": 0.258235,
     "end_time": "2024-10-16T01:19:00.929761",
     "exception": false,
     "start_time": "2024-10-16T01:19:00.671526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "missing_df['FGC-Season']",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:19:01.431519Z",
     "iopub.status.busy": "2024-10-16T01:19:01.431095Z",
     "iopub.status.idle": "2024-10-16T01:19:01.440615Z",
     "shell.execute_reply": "2024-10-16T01:19:01.43926Z"
    },
    "papermill": {
     "duration": 0.262073,
     "end_time": "2024-10-16T01:19:01.443303",
     "exception": false,
     "start_time": "2024-10-16T01:19:01.18123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import VotingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import cohen_kappa_score\nfrom scipy.optimize import minimize\nfrom tqdm import tqdm\nfrom IPython.display import clear_output\n\n# Function definitions (unchanged)\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    # Ensure all categorical columns are encoded\n    for col in categorical_features:\n        X[col] = pd.factorize(X[col])[0].astype('int64')\n    \n    test_data = test_data.copy()\n    for col in categorical_features:\n        test_data[col] = pd.factorize(test_data[col])[0].astype('int64')\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {tKappa:.3f}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission\n\n# Model parameters and instantiation (unchanged)\nParams = {\n    'learning_rate': 0.046,\n    'max_depth': 12,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,\n    'lambda_l2': 0.01\n}\n\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,\n    'reg_lambda': 5,\n    'random_state': SEED\n}\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'cat_features': cat_c,\n    'verbose': 0,\n    'l2_leaf_reg': 10\n}\n\nLight = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n\n# Combine models using Voting Regressor\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n])\n\n# Train the ensemble model\nSubmission = TrainML(voting_model, test)\n\n# Save submission\n# Submission.to_csv('submission.csv', index=False)\n",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:19:02.006254Z",
     "iopub.status.busy": "2024-10-16T01:19:02.005807Z",
     "iopub.status.idle": "2024-10-16T01:19:55.08701Z",
     "shell.execute_reply": "2024-10-16T01:19:55.085201Z"
    },
    "papermill": {
     "duration": 53.335039,
     "end_time": "2024-10-16T01:19:55.090628",
     "exception": false,
     "start_time": "2024-10-16T01:19:01.755589",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "test.shape",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:19:55.617618Z",
     "iopub.status.busy": "2024-10-16T01:19:55.61705Z",
     "iopub.status.idle": "2024-10-16T01:19:55.638524Z",
     "shell.execute_reply": "2024-10-16T01:19:55.636873Z"
    },
    "papermill": {
     "duration": 0.292787,
     "end_time": "2024-10-16T01:19:55.641456",
     "exception": false,
     "start_time": "2024-10-16T01:19:55.348669",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport pandas as pd\nimport os\nimport re\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nSEED = 42\nn_splits = 5\n\n# Load datasets\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\n\n\n\ndef process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\n\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    stats, indexes = zip(*results)\n    \n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    return df\n\n        \ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\ntrain = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)   \n\n\n##########\n\n# import pandas as pd\n# from sklearn.impute import KNNImputer\n\n# # Assuming 'train' is your DataFrame\n\n# # Step 1: Create the KNN imputer\n# imputer = KNNImputer(n_neighbors=5)  # You can adjust the number of neighbors\n\n# # Step 2: Select numeric columns and fit the imputer\n# numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n# imputed_data = imputer.fit_transform(train[numeric_cols])\n\n# # Step 3: Create a new DataFrame with the imputed values\n# train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n\n# # Step 4: Convert the 'sii' column back to integers\n# train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n\n# # If there are other columns to retain, you can merge them back\n# for col in train.columns:\n#     if col not in numeric_cols:\n#         train_imputed[col] = train[col]\n\n# # Now, check if 'sii' has been filled and is of integer type\n# print(train_imputed['sii'].isna().sum())  # Should be 0 if all NaNs were filled\n# print(train_imputed['sii'].dtype)  # Should show 'int'\n\n# train_imputed['sii']=train['sii']\n\n# train=train_imputed\n\n\n#########\n\n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n        \ntrain = update(train)\ntest = update(test)\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\nfor col in cat_c:\n    mapping = create_mapping(col, train)\n    mappingTe = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mappingTe).astype(int)\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission\n\n# Model parameters for LightGBM\nParams = {\n    'learning_rate': 0.046,\n    'max_depth': 12,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,  # Increased from 6.59\n    'lambda_l2': 0.01  # Increased from 2.68e-06\n}\n\n\n# XGBoost parameters\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,  # Increased from 0.1\n    'reg_lambda': 5,  # Increased from 1\n    'random_state': SEED\n}\n\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_seed': SEED,\n    'cat_features': cat_c,\n    'verbose': 0,\n    'l2_leaf_reg': 10  # Increase this value\n}\n\n# Create model instances\nLight = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n\n# Combine models using Voting Regressor\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n])\n\n# Train the ensemble model\nSubmission1 = TrainML(voting_model, test)\n\n# Save submission\n#Submission1.to_csv('submission.csv', index=False)\n",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:19:56.227977Z",
     "iopub.status.busy": "2024-10-16T01:19:56.227363Z",
     "iopub.status.idle": "2024-10-16T01:24:05.292317Z",
     "shell.execute_reply": "2024-10-16T01:24:05.290797Z"
    },
    "papermill": {
     "duration": 249.381649,
     "end_time": "2024-10-16T01:24:05.295075",
     "exception": false,
     "start_time": "2024-10-16T01:19:55.913426",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "Submission1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:24:05.853088Z",
     "iopub.status.busy": "2024-10-16T01:24:05.852592Z",
     "iopub.status.idle": "2024-10-16T01:24:05.866479Z",
     "shell.execute_reply": "2024-10-16T01:24:05.865283Z"
    },
    "papermill": {
     "duration": 0.264925,
     "end_time": "2024-10-16T01:24:05.869189",
     "exception": false,
     "start_time": "2024-10-16T01:24:05.604264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "papermill": {
     "duration": 0.247397,
     "end_time": "2024-10-16T01:24:06.366487",
     "exception": false,
     "start_time": "2024-10-16T01:24:06.11909",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import numpy as np\nimport pandas as pd\nimport os\nimport re\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nSEED = 42\nn_splits = 5\n\n# Load datasets\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ndef process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    stats, indexes = zip(*results)\n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    return df\n\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\ntrain = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\n\n#####\n\n\n# import pandas as pd\n# from sklearn.impute import KNNImputer\n\n# # Assuming 'train' is your DataFrame\n\n# # Step 1: Create the KNN imputer\n# imputer = KNNImputer(n_neighbors=5)  # You can adjust the number of neighbors\n\n# # Step 2: Select numeric columns and fit the imputer\n# numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n# imputed_data = imputer.fit_transform(train[numeric_cols])\n\n# # Step 3: Create a new DataFrame with the imputed values\n# train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n\n# # Step 4: Convert the 'sii' column back to integers\n# train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n\n# # If there are other columns to retain, you can merge them back\n# for col in train.columns:\n#     if col not in numeric_cols:\n#         train_imputed[col] = train[col]\n\n# # Now, check if 'sii' has been filled and is of integer type\n# print(train_imputed['sii'].isna().sum())  # Should be 0 if all NaNs were filled\n# print(train_imputed['sii'].dtype)  # Should show 'int'\n\n# train_imputed['sii']=train['sii']\n\n# train=train_imputed\n\n#####\n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-Season', 'CGAS-CGAS_Score',\n                'Physical-Season', 'Physical-BMI', 'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'Fitness_Endurance-Season',\n                'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-Season',\n                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone',\n                'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone',\n                'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat',\n                'BIA-BIA_Frame_num', 'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'BIA-BIA_TBW', 'PAQ_A-Season',\n                'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T',\n                'PreInt_EduHx-Season', 'PreInt_EduHx-computerinternet_hoursday', 'sii']\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', 'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n         'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n\ntrain = update(train)\ntest = update(test)\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\nfor col in cat_c:\n    mapping = create_mapping(col, train)\n    mappingTe = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mappingTe).astype(int)\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tp_rounded = threshold_Rounder(tpm, KappaOPtimizer.x)\n\n    return tp_rounded\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\n# Imputation step: Filling missing values with the median\nimputer = SimpleImputer(strategy='median')\n\n# Updating the ensemble to include the RandomForest and GradientBoosting models\nensemble = VotingRegressor(estimators=[\n    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n])\n\n# Train the ensemble with the updated model pipeline\npredictions = TrainML(ensemble, test)\n\n# Save predictions to a CSV file\nsample['sii'] = predictions\n#sample.to_csv('submission.csv', index=False)",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:24:06.862833Z",
     "iopub.status.busy": "2024-10-16T01:24:06.86234Z",
     "iopub.status.idle": "2024-10-16T01:29:18.973744Z",
     "shell.execute_reply": "2024-10-16T01:29:18.972214Z"
    },
    "papermill": {
     "duration": 312.364146,
     "end_time": "2024-10-16T01:29:18.976347",
     "exception": false,
     "start_time": "2024-10-16T01:24:06.612201",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "sample",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:29:19.478882Z",
     "iopub.status.busy": "2024-10-16T01:29:19.47834Z",
     "iopub.status.idle": "2024-10-16T01:29:19.491495Z",
     "shell.execute_reply": "2024-10-16T01:29:19.490299Z"
    },
    "papermill": {
     "duration": 0.266453,
     "end_time": "2024-10-16T01:29:19.493911",
     "exception": false,
     "start_time": "2024-10-16T01:29:19.227458",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "import pandas as pd\n\n# Load your three submission files\nsub1 = Submission\nsub2 = Submission1\nsub3 = sample\n\n# Ensure the IDs are aligned (if not sorted)\nsub1 = sub1.sort_values(by='id').reset_index(drop=True)\nsub2 = sub2.sort_values(by='id').reset_index(drop=True)\nsub3 = sub3.sort_values(by='id').reset_index(drop=True)\n\n# Combine the three predictions\ncombined = pd.DataFrame({\n    'id': sub1['id'],\n    'sii_1': sub1['sii'],\n    'sii_2': sub2['sii'],\n    'sii_3': sub3['sii']\n})\n\n# Apply majority voting\ndef majority_vote(row):\n    return row.mode()[0]  # Mode gets the most frequent value (majority vote)\n\ncombined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3']].apply(majority_vote, axis=1)\n\n# Create the final submission DataFrame\nfinal_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n\n# Save the final submission to a CSV file\nfinal_submission.to_csv('submission.csv', index=False)\n\nprint(\"Majority voting completed and saved to 'Final_Submission.csv'\")",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:29:19.989998Z",
     "iopub.status.busy": "2024-10-16T01:29:19.988884Z",
     "iopub.status.idle": "2024-10-16T01:29:20.01571Z",
     "shell.execute_reply": "2024-10-16T01:29:20.01427Z"
    },
    "papermill": {
     "duration": 0.277622,
     "end_time": "2024-10-16T01:29:20.018389",
     "exception": false,
     "start_time": "2024-10-16T01:29:19.740767",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# # Load your three submission files\n# sub1 = Submission\n# sub2 = Submission1\n# sub3 = sample\n\n# sub1 = sub1.sort_values(by='id').reset_index(drop=True)\n# sub2 = sub2.sort_values(by='id').reset_index(drop=True)\n# sub3 = sub3.sort_values(by='id').reset_index(drop=True)\n\n# sub1 = sub1.rename(columns={'sii': 'sii_1'})\n# sub2 = sub2.rename(columns={'sii': 'sii_2'})\n# sub3 = sub3.rename(columns={'sii': 'sii_3'})\n# subs = pd.merge(sub1,sub2,on=['id'])\n# subs = pd.merge(subs,sub3,on=['id'])\n\n# #subs['sii_s'] = subs['sii_1'] *0.555 + 0.001* subs['sii_2'] + 0.444* subs['sii_3']\n# #subs['sii_s'] = subs['sii_1'] *0.450 + 0.450* subs['sii_2'] + 0.100* subs['sii_3']\n# #subs['sii_s'] = subs['sii_1'] *0.525 + 0.050* subs['sii_2'] + 0.425* subs['sii_3']\n# #subs['sii_s'] = subs['sii_1'] *0.500 + 0.100* subs['sii_2'] + 0.400* subs['sii_3']\n\n# subs['sii_s'] = np.round(subs['sii_1'] *0.85 + 0.10* subs['sii_2'] + 0.05* subs['sii_3'])\n\n# subs['sii_s'] = subs['sii_s'].astype(int)\n\n# combined = pd.DataFrame({\n    \n#     'id'   : sub1['id'],\n    \n#     'sii_1': sub1['sii_1'],\n#     'sii_2': sub2['sii_2'],\n#     'sii_3': sub3['sii_3'],\n    \n#     'sii_s': subs['sii_s'],\n# })\n\n# display(combined)\n\n# def majority_vote(row):\n#     return row.mode()[0]\n                                                         \n# combined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3', 'sii_s']].apply(majority_vote, axis=1)\n\n# final_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n\n# final_submission.to_csv('submission.csv', index=False)\n\n# print(\"Majority voting completed and saved to 'Final_Submission.csv'\")",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:29:20.525059Z",
     "iopub.status.busy": "2024-10-16T01:29:20.523954Z",
     "iopub.status.idle": "2024-10-16T01:29:20.531334Z",
     "shell.execute_reply": "2024-10-16T01:29:20.530066Z"
    },
    "papermill": {
     "duration": 0.261198,
     "end_time": "2024-10-16T01:29:20.533686",
     "exception": false,
     "start_time": "2024-10-16T01:29:20.272488",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# X = train.drop(['sii'], axis=1)\n# y = train['sii']",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:29:21.038982Z",
     "iopub.status.busy": "2024-10-16T01:29:21.038545Z",
     "iopub.status.idle": "2024-10-16T01:29:21.043498Z",
     "shell.execute_reply": "2024-10-16T01:29:21.042337Z"
    },
    "papermill": {
     "duration": 0.262815,
     "end_time": "2024-10-16T01:29:21.046072",
     "exception": false,
     "start_time": "2024-10-16T01:29:20.783257",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# X_train=X\n# y_train=y",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:29:21.555384Z",
     "iopub.status.busy": "2024-10-16T01:29:21.554954Z",
     "iopub.status.idle": "2024-10-16T01:29:21.560271Z",
     "shell.execute_reply": "2024-10-16T01:29:21.55911Z"
    },
    "papermill": {
     "duration": 0.263534,
     "end_time": "2024-10-16T01:29:21.562687",
     "exception": false,
     "start_time": "2024-10-16T01:29:21.299153",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "# from sklearn.model_selection import RandomizedSearchCV\n# from sklearn.impute import SimpleImputer\n# from sklearn.pipeline import Pipeline\n# from sklearn.ensemble import VotingRegressor\n# from lightgbm import LGBMRegressor\n# from xgboost import XGBRegressor\n# from catboost import CatBoostRegressor\n# from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\n# # Define hyperparameter grids for each model\n\n# # LightGBM hyperparameter grid\n# lgb_param_grid = {\n#     'regressor__learning_rate': [0.01, 0.03, 0.046, 0.1],\n#     'regressor__max_depth': [8, 12, 16],\n#     'regressor__num_leaves': [128, 256, 478, 512],\n#     'regressor__min_data_in_leaf': [10, 13, 16],\n#     'regressor__feature_fraction': [0.7, 0.8, 0.893],\n#     'regressor__bagging_fraction': [0.7, 0.784, 0.85],\n#     'regressor__lambda_l1': [5, 10, 15],\n#     'regressor__lambda_l2': [0.001, 0.01, 0.1]\n# }\n\n# # XGBoost hyperparameter grid\n# xgb_param_grid = {\n#     'regressor__learning_rate': [0.01, 0.05, 0.1],\n#     'regressor__max_depth': [4, 6, 8],\n#     'regressor__n_estimators': [100, 200, 300],\n#     'regressor__subsample': [0.7, 0.8, 0.9],\n#     'regressor__colsample_bytree': [0.7, 0.8, 0.9],\n#     'regressor__reg_alpha': [0.1, 1, 5],\n#     'regressor__reg_lambda': [1, 5, 10]\n# }\n\n# # CatBoost hyperparameter grid\n# cat_param_grid = {\n#     'regressor__learning_rate': [0.03, 0.05, 0.07],\n#     'regressor__depth': [4, 6, 8],\n#     'regressor__iterations': [200, 300, 400],\n#     'regressor__l2_leaf_reg': [3, 10, 15]\n# }\n\n# # RandomForest hyperparameter grid\n# rf_param_grid = {\n#     'regressor__n_estimators': [100, 200, 300],\n#     'regressor__max_depth': [10, 20, 30],\n#     'regressor__min_samples_split': [2, 5, 10],\n#     'regressor__min_samples_leaf': [1, 2, 4]\n# }\n\n# # GradientBoosting hyperparameter grid\n# gb_param_grid = {\n#     'regressor__learning_rate': [0.01, 0.05, 0.1],\n#     'regressor__n_estimators': [100, 200, 300],\n#     'regressor__max_depth': [3, 5, 7],\n#     'regressor__subsample': [0.7, 0.8, 0.9]\n# }\n\n# # Imputer for handling missing values\n# imputer = SimpleImputer(strategy='median')\n\n# # Pipelines for each regressor\n# lgb_pipeline = Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])\n# xgb_pipeline = Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])\n# cat_pipeline = Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])\n# rf_pipeline = Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])\n# gb_pipeline = Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))])\n\n# # Perform RandomizedSearchCV for each model\n\n# # LightGBM\n# lgb_search = RandomizedSearchCV(lgb_pipeline, lgb_param_grid, n_iter=20, scoring='neg_mean_squared_error', cv=3, random_state=SEED)\n# lgb_search.fit(X_train, y_train)\n\n# # XGBoost\n# xgb_search = RandomizedSearchCV(xgb_pipeline, xgb_param_grid, n_iter=20, scoring='neg_mean_squared_error', cv=3, random_state=SEED)\n# xgb_search.fit(X_train, y_train)\n\n# # CatBoost\n# cat_search = RandomizedSearchCV(cat_pipeline, cat_param_grid, n_iter=20, scoring='neg_mean_squared_error', cv=3, random_state=SEED)\n# cat_search.fit(X_train, y_train)\n\n# # RandomForest\n# rf_search = RandomizedSearchCV(rf_pipeline, rf_param_grid, n_iter=20, scoring='neg_mean_squared_error', cv=3, random_state=SEED)\n# rf_search.fit(X_train, y_train)\n\n# # GradientBoosting\n# gb_search = RandomizedSearchCV(gb_pipeline, gb_param_grid, n_iter=20, scoring='neg_mean_squared_error', cv=3, random_state=SEED)\n# gb_search.fit(X_train, y_train)\n\n# # Get the best models from the RandomizedSearchCV results\n# best_lgb = lgb_search.best_estimator_\n# best_xgb = xgb_search.best_estimator_\n# best_cat = cat_search.best_estimator_\n# best_rf = rf_search.best_estimator_\n# best_gb = gb_search.best_estimator_\n\n# # Create a new ensemble with the best estimators\n# ensemble = VotingRegressor(estimators=[\n#     ('lgb', best_lgb),\n#     ('xgb', best_xgb),\n#     ('cat', best_cat),\n#     ('rf', best_rf),\n#     ('gb', best_gb)\n# ])\n\n# # Train the ensemble\n# predictions = TrainML(ensemble, test)\n\n# # Save predictions to a CSV file\n# sample['sii'] = predictions\n# sample.to_csv('submission.csv', index=False)",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-10-16T01:29:22.127359Z",
     "iopub.status.busy": "2024-10-16T01:29:22.126891Z",
     "iopub.status.idle": "2024-10-16T01:29:22.136974Z",
     "shell.execute_reply": "2024-10-16T01:29:22.135776Z"
    },
    "papermill": {
     "duration": 0.265233,
     "end_time": "2024-10-16T01:29:22.139391",
     "exception": false,
     "start_time": "2024-10-16T01:29:21.874158",
     "status": "completed"
    },
    "tags": [],
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": "",
   "metadata": {
    "papermill": {
     "duration": 0.252327,
     "end_time": "2024-10-16T01:29:22.64439",
     "exception": false,
     "start_time": "2024-10-16T01:29:22.392063",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "execution_count": null
  }
 ]
}
