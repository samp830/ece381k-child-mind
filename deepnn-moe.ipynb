{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================\n# Import Necessary Libraries\n# ==============================================\nimport numpy as np\nimport pandas as pd\nimport os\nimport re\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nimport polars as pl\nimport polars.selectors as cs\nimport matplotlib.pyplot as plt\nfrom matplotlib.ticker import MaxNLocator, FormatStrFormatter, PercentFormatter\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.pipeline import Pipeline\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom scipy.optimize import minimize\nfrom sklearn.impute import KNNImputer\n\nfrom scipy.stats import mode  # For majority voting\n\n# PyTorch Libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Suppress warnings and set display options\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\n# Set random seed for reproducibility\nSEED = 0\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:32:47.853516Z","iopub.execute_input":"2024-12-05T17:32:47.853900Z","iopub.status.idle":"2024-12-05T17:32:54.680571Z","shell.execute_reply.started":"2024-12-05T17:32:47.853853Z","shell.execute_reply":"2024-12-05T17:32:54.679576Z"}},"outputs":[{"execution_count":1,"output_type":"execute_result","data":{"text/plain":"<torch._C.Generator at 0x7ab75b0b7c90>"},"metadata":{}}],"execution_count":1},{"cell_type":"code","source":"# def process_file(filename, dirname):\n#     df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n#     df.drop('step', axis=1, inplace=True)\n#     return df.describe().values.reshape(-1), filename.split('=')[1]\n\n# def load_time_series(dirname) -> pd.DataFrame:\n#     ids = os.listdir(dirname)\n    \n#     with ThreadPoolExecutor() as executor:\n#         results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n#     stats, indexes = zip(*results)\n    \n#     df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n#     df['id'] = indexes\n#     return df\n\n\n# class AutoEncoder(nn.Module):\n#     def __init__(self, input_dim, encoding_dim):\n#         super(AutoEncoder, self).__init__()\n#         self.encoder = nn.Sequential(\n#             nn.Linear(input_dim, encoding_dim*3),\n#             nn.ReLU(),\n#             nn.Linear(encoding_dim*3, encoding_dim*2),\n#             nn.ReLU(),\n#             nn.Linear(encoding_dim*2, encoding_dim),\n#             nn.ReLU()\n#         )\n#         self.decoder = nn.Sequential(\n#             nn.Linear(encoding_dim, input_dim*2),\n#             nn.ReLU(),\n#             nn.Linear(input_dim*2, input_dim*3),\n#             nn.ReLU(),\n#             nn.Linear(input_dim*3, input_dim),\n#             nn.Sigmoid()\n#         )\n        \n#     def forward(self, x):\n#         encoded = self.encoder(x)\n#         decoded = self.decoder(encoded)\n#         return decoded\n\n\n# def perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n#     scaler = StandardScaler()\n#     df_scaled = scaler.fit_transform(df)\n    \n#     data_tensor = torch.FloatTensor(df_scaled)\n    \n#     input_dim = data_tensor.shape[1]\n#     autoencoder = AutoEncoder(input_dim, encoding_dim)\n    \n#     criterion = nn.MSELoss()\n#     optimizer = optim.Adam(autoencoder.parameters())\n    \n#     for epoch in range(epochs):\n#         for i in range(0, len(data_tensor), batch_size):\n#             batch = data_tensor[i : i + batch_size]\n#             optimizer.zero_grad()\n#             reconstructed = autoencoder(batch)\n#             loss = criterion(reconstructed, batch)\n#             loss.backward()\n#             optimizer.step()\n            \n#         if (epoch + 1) % 10 == 0:\n#             print(f'Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}]')\n                 \n#     with torch.no_grad():\n#         encoded_data = autoencoder.encoder(data_tensor).numpy()\n        \n#     df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i + 1}' for i in range(encoded_data.shape[1])])\n    \n#     return df_encoded\n\n# def feature_engineering(df):\n#     season_cols = [col for col in df.columns if 'Season' in col]\n#     df = df.drop(season_cols, axis=1) \n#     df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n#     df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n#     df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n#     df['BFP_BMI'] = df['BIA-BIA_Fat'] / df['BIA-BIA_BMI']\n#     df['FFMI_BFP'] = df['BIA-BIA_FFMI'] / df['BIA-BIA_Fat']\n#     df['FMI_BFP'] = df['BIA-BIA_FMI'] / df['BIA-BIA_Fat']\n#     df['LST_TBW'] = df['BIA-BIA_LST'] / df['BIA-BIA_TBW']\n#     df['BFP_BMR'] = df['BIA-BIA_Fat'] * df['BIA-BIA_BMR']\n#     df['BFP_DEE'] = df['BIA-BIA_Fat'] * df['BIA-BIA_DEE']\n#     df['BMR_Weight'] = df['BIA-BIA_BMR'] / df['Physical-Weight']\n#     df['DEE_Weight'] = df['BIA-BIA_DEE'] / df['Physical-Weight']\n#     df['SMM_Height'] = df['BIA-BIA_SMM'] / df['Physical-Height']\n#     df['Muscle_to_Fat'] = df['BIA-BIA_SMM'] / df['BIA-BIA_FMI']\n#     df['Hydration_Status'] = df['BIA-BIA_TBW'] / df['Physical-Weight']\n#     df['ICW_TBW'] = df['BIA-BIA_ICW'] / df['BIA-BIA_TBW']\n    \n#     return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:32:54.682033Z","iopub.execute_input":"2024-12-05T17:32:54.682831Z","iopub.status.idle":"2024-12-05T17:32:54.690407Z","shell.execute_reply.started":"2024-12-05T17:32:54.682786Z","shell.execute_reply":"2024-12-05T17:32:54.689265Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# train = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\n# test = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n# sample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\n# train_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\n# test_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\n# df_train = train_ts.drop('id', axis=1)\n# df_test = test_ts.drop('id', axis=1)\n\n# train_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\n# test_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n\n# time_series_cols = train_ts_encoded.columns.tolist()\n# train_ts_encoded[\"id\"]=train_ts[\"id\"]\n# test_ts_encoded['id']=test_ts[\"id\"]\n\n# train = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n# test = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n\n# imputer = KNNImputer(n_neighbors=5)\n# numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n# imputed_data = imputer.fit_transform(train[numeric_cols])\n# train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n# train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n# for col in train.columns:\n#     if col not in numeric_cols:\n#         train_imputed[col] = train[col]\n        \n# train = train_imputed\n\n# train = feature_engineering(train)\n# train = train.dropna(thresh=10, axis=0)\n# test = feature_engineering(test)\n\n# train = train.drop('id', axis=1)\n# test  = test .drop('id', axis=1)   \n\n\n# featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n#                 'CGAS-CGAS_Score', 'Physical-BMI',\n#                 'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n#                 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n#                 'Fitness_Endurance-Max_Stage',\n#                 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n#                 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n#                 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n#                 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n#                 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n#                 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n#                 'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n#                 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n#                 'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n#                 'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n#                 'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n#                 'SDS-SDS_Total_T',\n#                 'PreInt_EduHx-computerinternet_hoursday', 'sii', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n#                 'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n#                 'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW']\n\n# featuresCols += time_series_cols\n\n# train = train[featuresCols]\n# train = train.dropna(subset='sii')\n\n# featuresCols = ['Basic_Demos-Age', 'Basic_Demos-Sex',\n#                 'CGAS-CGAS_Score', 'Physical-BMI',\n#                 'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n#                 'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n#                 'Fitness_Endurance-Max_Stage',\n#                 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n#                 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n#                 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n#                 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n#                 'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone',\n#                 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n#                 'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n#                 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n#                 'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n#                 'BIA-BIA_TBW', 'PAQ_A-PAQ_A_Total',\n#                 'PAQ_C-PAQ_C_Total', 'SDS-SDS_Total_Raw',\n#                 'SDS-SDS_Total_T',\n#                 'PreInt_EduHx-computerinternet_hoursday', 'BMI_Age','Internet_Hours_Age','BMI_Internet_Hours',\n#                 'BFP_BMI', 'FFMI_BFP', 'FMI_BFP', 'LST_TBW', 'BFP_BMR', 'BFP_DEE', 'BMR_Weight', 'DEE_Weight',\n#                 'SMM_Height', 'Muscle_to_Fat', 'Hydration_Status', 'ICW_TBW']\n\n# featuresCols += time_series_cols\n# test = test[featuresCols]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:32:54.693059Z","iopub.execute_input":"2024-12-05T17:32:54.693433Z","iopub.status.idle":"2024-12-05T17:32:54.704359Z","shell.execute_reply.started":"2024-12-05T17:32:54.693402Z","shell.execute_reply":"2024-12-05T17:32:54.703529Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# test_df = test\n# train_df = train","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:32:54.705521Z","iopub.execute_input":"2024-12-05T17:32:54.705897Z","iopub.status.idle":"2024-12-05T17:32:54.716898Z","shell.execute_reply.started":"2024-12-05T17:32:54.705866Z","shell.execute_reply":"2024-12-05T17:32:54.716081Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ==============================================\n# Load and Merge Data\n# ==============================================\n# Load main datasets\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n\n# ==============================================\n# Feature Engineering\n# ==============================================\ndef engineer_features(df):\n    \"\"\"\n    Create interaction features.\n    \"\"\"\n    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['Pulse_Pressure'] = df['Physical-Systolic_BP'] - df['Physical-Diastolic_BP']\n    df['HeartRate_Age'] = df['Physical-HeartRate'] * df['Basic_Demos-Age']\n    df['Fitness_Score'] = df['Fitness_Endurance-Max_Stage'] * (\n        df['Fitness_Endurance-Time_Mins'] * 60 + df['Fitness_Endurance-Time_Sec']\n    )\n    df['FMI_FFMI_Ratio'] = df['BIA-BIA_FMI'] / (df['BIA-BIA_FFMI'] + 1e-6)\n    df['Sleep_Internet_Hours'] = df['SDS-SDS_Total_T'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['Waist_Height_Ratio'] = df['Physical-Waist_Circumference'] / (df['Physical-Height'] + 1e-6)\n    return df\n\n# Apply feature engineering to both train and test datasets\ntrain = engineer_features(train)\ntest = engineer_features(test)\n\n# ==============================================\n# Define Feature Columns\n# ==============================================\nfeaturesCols = list(set(train.columns).intersection(set(test.columns)))\n# featuresCols = [\n#     'Internet_Hours_Age', 'Physical-Height', 'Sleep_Internet_Hours', 'Basic_Demos-Age', \n#     'BMI_Internet_Hours', 'HeartRate_Age', 'BMI_Age', 'Physical-Waist_Circumference', \n#     'FGC-FGC_CU', 'BIA-BIA_BMI', 'SDS-SDS_Total_Raw', 'FGC-FGC_GSND', 'Physical-BMI', \n#     'FGC-FGC_GSD', 'FGC-FGC_PU', 'BIA-BIA_FFMI', 'BIA-BIA_Frame_num', 'Physical-Systolic_BP', \n#     'Pulse_Pressure', 'FGC-FGC_TL', 'Basic_Demos-Sex', 'FGC-FGC_SRL_Zone', 'CGAS-CGAS_Score', \n#     'Fitness_Score', 'BIA-BIA_SMM', 'BIA-BIA_Activity_Level_num'\n# ]\n\n# Select the features from train and test datasets\ntrain_df = train[featuresCols + ['sii']]  # Include 'sii' in train features\ntest_df = test[featuresCols]\n\n# Drop 'id' from train and test data if present\ntrain_df = train_df.drop('id', axis=1, errors='ignore')\ntest_df = test_df.drop('id', axis=1, errors='ignore')\n\n# Drop rows with missing target variable 'sii' in train data\ntrain_df = train_df.dropna(subset=['sii'])\n\nfeaturesCols = list(test_df.columns)\n\n# ==============================================\n# Handle Categorical Variables\n# ==============================================\n# List of categorical columns\ncat_c = [\n    'Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n    'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n    'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season'\n]\n\ndef create_consistent_mapping(column):\n    \"\"\"\n    Create a consistent mapping for categorical variables by combining unique values from both datasets.\n    \"\"\"\n    combined_values = pd.concat([train_df[column], test_df[column]], axis=0).fillna('Missing')\n    unique_values = combined_values.unique()\n    sorted_values = np.sort(unique_values)  # Ensure consistent ordering\n    return {value: idx for idx, value in enumerate(sorted_values)}\n\n# Map categorical variables to integers using a consistent mapping\nfor col in cat_c:\n    mapping = create_consistent_mapping(col)\n    train_df[col] = train_df[col].fillna('Missing').map(mapping).astype(int)\n    test_df[col] = test_df[col].fillna('Missing').map(mapping).astype(int)\n\n\n# ==============================================\n# Handle Missing Values with KNN Imputer\n# ==============================================\n# Impute missing values using KNNImputer\nimputer = KNNImputer(n_neighbors=3)\nnumeric_cols = train_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n\n# Remove 'sii' from numeric_cols when applying to test data\nif 'sii' in numeric_cols:\n    numeric_cols.remove('sii')\n\n# Impute train data\ntrain_df[numeric_cols] = imputer.fit_transform(train_df[numeric_cols])\n\n# Impute test data\n# Ensure that all columns in numeric_cols exist in test data\nnumeric_cols_test = [col for col in numeric_cols if col in test_df.columns]\ntest_df[numeric_cols_test] = imputer.transform(test_df[numeric_cols_test])\n\n# Ensure 'sii' remains integer\ntrain_df['sii'] = train_df['sii'].round().astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:32:54.718492Z","iopub.execute_input":"2024-12-05T17:32:54.718804Z","iopub.status.idle":"2024-12-05T17:32:57.226883Z","shell.execute_reply.started":"2024-12-05T17:32:54.718777Z","shell.execute_reply":"2024-12-05T17:32:57.225318Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ==============================================\n# Model Training and Evaluation Functions\n# ==============================================\ndef quadratic_weighted_kappa(y_true, y_pred):\n    \"\"\"\n    Calculate Quadratic Weighted Kappa.\n    \"\"\"\n    y_true = y_true.astype(int)\n    y_pred = y_pred.astype(int)\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    \"\"\"\n    Apply thresholds to continuous predictions.\n    \"\"\"\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    \"\"\"\n    Objective function for optimizing thresholds.\n    \"\"\"\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\n# ==============================================\n# Split Features Among Expert Networks\n# ==============================================\n# feature_splits = np.array_split(featuresCols, 4)\n# features_expert1 = feature_splits[0].tolist()\n# features_expert2 = feature_splits[1].tolist()\n# features_expert3 = feature_splits[2].tolist()\n# features_expert4 = feature_splits[3].tolist()\nfeatures_expert1 = featuresCols\nfeatures_expert2 = featuresCols\nfeatures_expert3 = featuresCols\nfeatures_expert4 = featuresCols\n\n# ==============================================\n# Prepare Data for Training\n# ==============================================\n# Inputs for expert networks\nX_expert1 = train_df[features_expert1]\nX_expert2 = train_df[features_expert2]\nX_expert3 = train_df[features_expert3]\nX_expert4 = train_df[features_expert4]\n\n# Input for weighting network\nX_weighting = train_df[featuresCols]  # All features\n\n# Target variable\ny = train_df['sii']\n\n# Stratified Split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\ntrain_index, val_index = next(sss.split(X_weighting, y))\n\nX_expert_list_train = [X.iloc[train_index].reset_index(drop=True) for X in [X_expert1, X_expert2, X_expert3, X_expert4]]\nX_weighting_train = X_weighting.iloc[train_index].reset_index(drop=True)\ny_train = y.iloc[train_index].reset_index(drop=True)\n\nX_expert_list_val = [X.iloc[val_index].reset_index(drop=True) for X in [X_expert1, X_expert2, X_expert3, X_expert4]]\nX_weighting_val = X_weighting.iloc[val_index].reset_index(drop=True)\ny_val = y.iloc[val_index].reset_index(drop=True)\n\n# ==============================================\n# Define Custom Dataset Class\n# ==============================================\nclass MultiInputDataset(Dataset):\n    def __init__(self, X_expert_list, X_weighting, y=None):\n        self.X_expert_list = [torch.tensor(X.values, dtype=torch.float32) for X in X_expert_list]\n        self.X_weighting = torch.tensor(X_weighting.values, dtype=torch.float32)\n        if y is not None:\n            self.y = torch.tensor(y.values, dtype=torch.float32)\n        else:\n            self.y = None\n    def __len__(self):\n        return len(self.X_weighting)\n    def __getitem__(self, idx):\n        inputs = [X[idx] for X in self.X_expert_list]\n        weighting_input = self.X_weighting[idx]\n        if self.y is not None:\n            return inputs, weighting_input, self.y[idx]\n        else:\n            return inputs, weighting_input\n\n# Create datasets and data loaders\ntrain_dataset = MultiInputDataset(X_expert_list_train, X_weighting_train, y_train)\nval_dataset = MultiInputDataset(X_expert_list_val, X_weighting_val, y_val)\n\nbatch_size = 64\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# Prepare test data\nX_expert1_test = test_df[features_expert1]\nX_expert2_test = test_df[features_expert2]\nX_expert3_test = test_df[features_expert3]\nX_expert4_test = test_df[features_expert4]\nX_weighting_test = test_df[featuresCols]\n\ntest_dataset = MultiInputDataset(\n    [X_expert1_test.reset_index(drop=True), X_expert2_test.reset_index(drop=True), \n     X_expert3_test.reset_index(drop=True), X_expert4_test.reset_index(drop=True)],\n    X_weighting_test.reset_index(drop=True)\n)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# ==============================================\n# Define Neural Network Architectures\n# ==============================================\nclass ExpertNet(nn.Module):\n    def __init__(self, input_size):\n        super(ExpertNet, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 64),\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.BatchNorm1d(32),\n            nn.Linear(32, 1)\n        )\n    def forward(self, x):\n        return self.fc(x).squeeze()  # Output shape: [batch_size]\n\nclass WeightingNet(nn.Module):\n    def __init__(self, input_size):\n        super(WeightingNet, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 64),\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.BatchNorm1d(32),\n            nn.Linear(32, 4),\n            nn.Softmax(dim=1)  # Output weights that sum to 1\n        )\n    def forward(self, x):\n        return self.fc(x)  # Output shape: [batch_size, 4]\n\n# Initialize networks\ninput_sizes = [len(features_expert1), len(features_expert2), len(features_expert3), len(features_expert4)]\nexpert_nets = [ExpertNet(input_size) for input_size in input_sizes]\nweighting_net = WeightingNet(len(featuresCols))\n\n# Move models to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfor net in expert_nets:\n    net.to(device)\nweighting_net.to(device)\n\n# ==============================================\n# Define Loss Function and Optimizer\n# ==============================================\nclass OrdinalLoss(nn.Module):\n    def __init__(self):\n        super(OrdinalLoss, self).__init__()\n    def forward(self, preds, targets):\n        # preds: continuous predictions [batch_size]\n        # targets: true labels [batch_size]\n        # Compute loss that penalizes deviations more severely for larger errors\n        loss = torch.mean((preds - targets.float()) ** 2)\n        return loss\n\ncriterion = OrdinalLoss()\n\noptimizer = optim.Adam(\n    list(expert_nets[0].parameters()) +\n    list(expert_nets[1].parameters()) +\n    list(expert_nets[2].parameters()) +\n    list(expert_nets[3].parameters()) +\n    list(weighting_net.parameters()),\n    lr=0.001\n)\n\n# ==============================================\n# Training Loop\n# ==============================================\nnum_epochs = 38\n\nfor epoch in range(num_epochs):\n    expert_nets[0].train()\n    expert_nets[1].train()\n    expert_nets[2].train()\n    expert_nets[3].train()\n    weighting_net.train()\n    \n    running_loss = 0.0\n    for inputs_list, weighting_input, labels in train_loader:\n        optimizer.zero_grad()\n        \n        # Move data to device\n        inputs_list = [inp.to(device) for inp in inputs_list]\n        weighting_input = weighting_input.to(device)\n        labels = labels.to(device)\n        \n        # Get outputs from expert networks\n        expert_outputs = []\n        for i in range(4):\n            expert_outputs.append(expert_nets[i](inputs_list[i]))\n        expert_outputs = torch.stack(expert_outputs, dim=1)  # Shape: [batch_size, 4]\n        \n        # Get weights from weighting network\n        weights = weighting_net(weighting_input)  # Shape: [batch_size, 4]\n        \n        # Compute weighted sum\n        preds = (expert_outputs * weights).sum(dim=1)  # Shape: [batch_size]\n        \n        # Compute loss\n        loss = criterion(preds, labels)\n        \n        # Backpropagate\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * labels.size(0)\n    epoch_loss = running_loss / len(train_dataset)\n    \n    # Validation\n    expert_nets[0].eval()\n    expert_nets[1].eval()\n    expert_nets[2].eval()\n    expert_nets[3].eval()\n    weighting_net.eval()\n    val_preds = []\n    val_labels = []\n    with torch.no_grad():\n        for inputs_list, weighting_input, labels in val_loader:\n            # Move data to device\n            inputs_list = [inp.to(device) for inp in inputs_list]\n            weighting_input = weighting_input.to(device)\n            labels = labels.to(device)\n            \n            # Get outputs from expert networks\n            expert_outputs = []\n            for i in range(4):\n                expert_outputs.append(expert_nets[i](inputs_list[i]))\n            expert_outputs = torch.stack(expert_outputs, dim=1)  # Shape: [batch_size, 4]\n            \n            # Get weights from weighting network\n            weights = weighting_net(weighting_input)  # Shape: [batch_size, 4]\n            \n            # Compute weighted sum\n            preds = (expert_outputs * weights).sum(dim=1)  # Shape: [batch_size]\n            \n            val_preds.append(preds.cpu().numpy())\n            val_labels.append(labels.cpu().numpy())\n    val_preds = np.concatenate(val_preds)\n    val_labels = np.concatenate(val_labels)\n    \n    # Compute validation loss and QWK\n    val_loss = np.mean((val_preds - val_labels) ** 2)\n    val_preds_rounded = np.clip(np.round(val_preds), 0, 3).astype(int)\n    val_kappa = quadratic_weighted_kappa(val_labels, val_preds_rounded)\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val QWK: {val_kappa:.4f}\")\n\n# ==============================================\n# Optimize Thresholds\n# ==============================================\nfrom scipy.optimize import minimize\n\n# Optimize thresholds\nKappaOptimizer = minimize(evaluate_predictions,\n                          x0=[0.5, 1.5, 2.5], args=(val_labels, val_preds), \n                          method='Nelder-Mead')\nbest_thresholds = KappaOptimizer.x\n\n# Evaluate optimized QWK\nval_preds_tuned = threshold_Rounder(val_preds, best_thresholds)\nval_kappa_tuned = quadratic_weighted_kappa(val_labels, val_preds_tuned)\nprint(f\"Optimized Validation QWK: {val_kappa_tuned:.4f}\")\n\n# ==============================================\n# Generate Predictions on Test Data\n# ==============================================\nexpert_nets[0].eval()\nexpert_nets[1].eval()\nexpert_nets[2].eval()\nexpert_nets[3].eval()\nweighting_net.eval()\n\ntest_preds = []\nwith torch.no_grad():\n    for inputs_list, weighting_input in test_loader:\n        # Move data to device\n        inputs_list = [inp.to(device) for inp in inputs_list]\n        weighting_input = weighting_input.to(device)\n        \n        # Get outputs from expert networks\n        expert_outputs = []\n        for i in range(4):\n            expert_outputs.append(expert_nets[i](inputs_list[i]))\n        expert_outputs = torch.stack(expert_outputs, dim=1)  # Shape: [batch_size, 4]\n        \n        # Get weights from weighting network\n        weights = weighting_net(weighting_input)  # Shape: [batch_size, 4]\n        \n        # Compute weighted sum\n        preds = (expert_outputs * weights).sum(dim=1)  # Shape: [batch_size]\n        \n        test_preds.append(preds.cpu().numpy())\ntest_preds = np.concatenate(test_preds)\n\n# Apply optimized thresholds\ntest_preds_tuned = threshold_Rounder(test_preds, best_thresholds)\ntest_preds_tuned = test_preds_tuned.astype(int)\n\n# Create submission DataFrame\nsubmission4 = pd.DataFrame({\n    'id': test['id'],\n    'sii': test_preds_tuned\n})\n\n# Save submission\nsubmission4.to_csv('submission.csv', index=False)\nprint(\"Submission file 'submission.csv' has been created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:32:57.228939Z","iopub.execute_input":"2024-12-05T17:32:57.229331Z","iopub.status.idle":"2024-12-05T17:33:08.887606Z","shell.execute_reply.started":"2024-12-05T17:32:57.229290Z","shell.execute_reply":"2024-12-05T17:33:08.886612Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/38, Loss: 0.6123, Val Loss: 0.5432, Val QWK: 0.2895\nEpoch 2/38, Loss: 0.4945, Val Loss: 0.5596, Val QWK: 0.2943\nEpoch 3/38, Loss: 0.4925, Val Loss: 0.5034, Val QWK: 0.3438\nEpoch 4/38, Loss: 0.4955, Val Loss: 0.4675, Val QWK: 0.3503\nEpoch 5/38, Loss: 0.4714, Val Loss: 0.4941, Val QWK: 0.3155\nEpoch 6/38, Loss: 0.4665, Val Loss: 0.5538, Val QWK: 0.2701\nEpoch 7/38, Loss: 0.4779, Val Loss: 0.4978, Val QWK: 0.3264\nEpoch 8/38, Loss: 0.4622, Val Loss: 0.4804, Val QWK: 0.3232\nEpoch 9/38, Loss: 0.4619, Val Loss: 0.4896, Val QWK: 0.2842\nEpoch 10/38, Loss: 0.4429, Val Loss: 0.5123, Val QWK: 0.3278\nEpoch 11/38, Loss: 0.4556, Val Loss: 0.5330, Val QWK: 0.3372\nEpoch 12/38, Loss: 0.4512, Val Loss: 0.5320, Val QWK: 0.3312\nEpoch 13/38, Loss: 0.4513, Val Loss: 0.4710, Val QWK: 0.3567\nEpoch 14/38, Loss: 0.4528, Val Loss: 0.4919, Val QWK: 0.3343\nEpoch 15/38, Loss: 0.4483, Val Loss: 0.4901, Val QWK: 0.3764\nEpoch 16/38, Loss: 0.4395, Val Loss: 0.5310, Val QWK: 0.2800\nEpoch 17/38, Loss: 0.4453, Val Loss: 0.5336, Val QWK: 0.2784\nEpoch 18/38, Loss: 0.4422, Val Loss: 0.4586, Val QWK: 0.3727\nEpoch 19/38, Loss: 0.4444, Val Loss: 0.4829, Val QWK: 0.4000\nEpoch 20/38, Loss: 0.4418, Val Loss: 0.5360, Val QWK: 0.2809\nEpoch 21/38, Loss: 0.4428, Val Loss: 0.4799, Val QWK: 0.3194\nEpoch 22/38, Loss: 0.4473, Val Loss: 0.4760, Val QWK: 0.3558\nEpoch 23/38, Loss: 0.4458, Val Loss: 0.5220, Val QWK: 0.2875\nEpoch 24/38, Loss: 0.4373, Val Loss: 0.4631, Val QWK: 0.3597\nEpoch 25/38, Loss: 0.4371, Val Loss: 0.4853, Val QWK: 0.3519\nEpoch 26/38, Loss: 0.4299, Val Loss: 0.4985, Val QWK: 0.3486\nEpoch 27/38, Loss: 0.4307, Val Loss: 0.4968, Val QWK: 0.2993\nEpoch 28/38, Loss: 0.4234, Val Loss: 0.4824, Val QWK: 0.3885\nEpoch 29/38, Loss: 0.4316, Val Loss: 0.9846, Val QWK: 0.3783\nEpoch 30/38, Loss: 0.4225, Val Loss: 0.4946, Val QWK: 0.3540\nEpoch 31/38, Loss: 0.4227, Val Loss: 0.4983, Val QWK: 0.3321\nEpoch 32/38, Loss: 0.4283, Val Loss: 0.4944, Val QWK: 0.3569\nEpoch 33/38, Loss: 0.4299, Val Loss: 0.5038, Val QWK: 0.3439\nEpoch 34/38, Loss: 0.4209, Val Loss: 0.4898, Val QWK: 0.3631\nEpoch 35/38, Loss: 0.4155, Val Loss: 0.4644, Val QWK: 0.3800\nEpoch 36/38, Loss: 0.4226, Val Loss: 0.5494, Val QWK: 0.3726\nEpoch 37/38, Loss: 0.4168, Val Loss: 0.5942, Val QWK: 0.3446\nEpoch 38/38, Loss: 0.4143, Val Loss: 0.4638, Val QWK: 0.3907\nOptimized Validation QWK: 0.4566\nSubmission file 'submission.csv' has been created.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"submission4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-05T17:33:08.888975Z","iopub.execute_input":"2024-12-05T17:33:08.889633Z","iopub.status.idle":"2024-12-05T17:33:08.900678Z","shell.execute_reply.started":"2024-12-05T17:33:08.889585Z","shell.execute_reply":"2024-12-05T17:33:08.899751Z"}},"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"          id  sii\n0   00008ff9    0\n1   000fd460    0\n2   00105258    1\n3   00115b9f    0\n4   0016bb22    1\n5   001f3379    0\n6   0038ba98    0\n7   0068a485    0\n8   0069fbed    1\n9   0083e397    2\n10  0087dd65    1\n11  00abe655    0\n12  00ae59c9    1\n13  00af6387    1\n14  00bd4359    1\n15  00c0cd71    1\n16  00d56d4b    0\n17  00d9913d    0\n18  00e6167c    0\n19  00ebc35d    1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>sii</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>00008ff9</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>000fd460</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>00105258</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>00115b9f</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0016bb22</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>001f3379</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0038ba98</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0068a485</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0069fbed</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0083e397</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0087dd65</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>00abe655</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>00ae59c9</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>00af6387</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>00bd4359</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>00c0cd71</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>00d56d4b</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>00d9913d</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>18</th>\n      <td>00e6167c</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>19</th>\n      <td>00ebc35d</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}