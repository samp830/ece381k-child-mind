{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy.stats import kurtosis\nimport numpy as np\nimport polars as pl\nimport pandas as pd\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nfrom sklearn.base import clone\nfrom copy import deepcopy\nimport optuna\nfrom scipy.optimize import minimize\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nfrom colorama import Fore, Style\n\nfrom tqdm import tqdm\nfrom IPython.display import clear_output\nfrom concurrent.futures import ThreadPoolExecutor\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nimport lightgbm as lgb\nfrom catboost import CatBoostRegressor, CatBoostClassifier\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import VotingRegressor\nfrom sklearn.model_selection import *\nfrom sklearn.metrics import *\n\nSEED = 42\nn_splits = 5\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport re\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\n\ndef process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n\n    # Convert 'time_of_day' to datetime\n    df['time_of_day'] = pd.to_datetime(df['time_of_day'])\n    df.set_index('time_of_day', inplace=True)\n\n    # Daily aggregation\n    daily_features = df.resample('D').agg({\n        'X': ['mean', 'std', 'max', 'min', 'median', 'skew', lambda x: kurtosis(x, nan_policy='omit')],\n        'Y': ['mean', 'std', 'max', 'min', 'median', 'skew', lambda x: kurtosis(x, nan_policy='omit')],\n        'Z': ['mean', 'std', 'max', 'min', 'median', 'skew', lambda x: kurtosis(x, nan_policy='omit')],\n        'enmo': ['mean', 'std', 'max', 'min', 'median', 'skew', lambda x: kurtosis(x, nan_policy='omit')],\n        'light': ['mean', 'std'],\n        'battery_voltage': ['mean', 'std'],\n        'non-wear_flag': 'sum'\n    })\n\n    # Flatten the MultiIndex columns\n    daily_features.columns = ['_'.join(col).strip() for col in daily_features.columns.values]\n    \n    # Add additional features\n    daily_features['activity_count'] = (df['enmo'] > 0.1).resample('D').sum()  # Adjust threshold as needed\n    daily_features['days_active'] = (df['non-wear_flag'] == 0).resample('D').sum()  # Count active days\n    \n    # Rate of change features\n    daily_features['enmo_change'] = daily_features['enmo_mean'].diff()\n    \n    # Rolling statistics\n    rolling_windows = [5, 10, 15]  # Days\n    for window in rolling_windows:\n        daily_features[f'enmo_rolling_mean_{window}'] = daily_features['enmo_mean'].rolling(window=window).mean()\n        daily_features[f'enmo_rolling_std_{window}'] = daily_features['enmo_std'].rolling(window=window).std()\n\n    # Frequency domain features using FFT\n    for axis in ['X', 'Y', 'Z', 'enmo']:\n        freq_features = np.fft.fft(df[axis])\n        daily_features[f'{axis}_dominant_freq'] = np.abs(freq_features).argmax()\n\n    # Reset index to retain 'id'\n    daily_features.reset_index(inplace=True)\n    \n    # Extract child ID from filename\n    child_id = filename.split('=')[1]\n    daily_features['id'] = child_id\n\n    return daily_features\n\n\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    # Concatenate all results into a single DataFrame\n    features_df = pd.concat(results, ignore_index=True)\n\n    return features_df\n\n# Build the autoencoder model\ndef build_autoencoder(input_dim, encoding_dim):\n    input_layer = Input(shape=(input_dim,))\n    \n    # Encoder: compressing the input\n    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n    \n    # Decoder: reconstructing the input\n    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n    \n    # Autoencoder model\n    autoencoder = Model(inputs=input_layer, outputs=decoded)\n    \n    # Encoder model (for getting the compressed representation)\n    encoder = Model(inputs=input_layer, outputs=encoded)\n    \n    autoencoder.compile(optimizer=Adam(), loss='mse')\n    \n    return autoencoder, encoder\n\n# Function to perform dimensionality reduction using an autoencoder\ndef perform_autoencoder(df, encoding_dim=50, epochs=50, batch_size=32):\n    \"\"\"\n    Perform dimensionality reduction using an Autoencoder.\n\n    Parameters:\n    df (pd.DataFrame): The input DataFrame with numerical features.\n    encoding_dim (int): The dimension of the encoded space.\n    epochs (int): Number of epochs to train the autoencoder.\n    batch_size (int): Size of the batches for training.\n\n    Returns:\n    pd.DataFrame: DataFrame containing the reduced-dimensional representation (encoding).\n    \"\"\"\n    \n    # Step 1: Standardize the data\n    scaler = StandardScaler()\n    df_scaled = scaler.fit_transform(df)\n    \n    # Step 2: Build the autoencoder\n    input_dim = df_scaled.shape[1]\n    autoencoder, encoder = build_autoencoder(input_dim, encoding_dim)\n    \n    # Step 3: Train the autoencoder\n    autoencoder.fit(df_scaled, df_scaled, epochs=epochs, batch_size=batch_size, shuffle=True, verbose=1)\n    \n    # Step 4: Get the encoded (reduced) representation\n    encoded_data = encoder.predict(df_scaled)\n    \n    # Step 5: Create a DataFrame for the encoded features\n    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i+1}' for i in range(encoded_data.shape[1])])\n    \n    return df_encoded\n\n# Load the data\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\n# Columns to drop\ncolumns_to_drop = [\n    'enmo_change',\n    'enmo_rolling_mean_5',\n    'enmo_rolling_std_5',\n    'enmo_rolling_mean_10',\n    'enmo_rolling_std_10',\n    'enmo_rolling_mean_15',\n    'enmo_rolling_std_15'\n]\n\n# Drop the columns\ntrain_ts.drop(columns=columns_to_drop, inplace=True)\n\n# Drop the columns\ntest_ts.drop(columns=columns_to_drop, inplace=True)\n\n# Drop 'id' column for training\ndf_train = train_ts.drop('id', axis=1)\ndf_test = test_ts.drop('id', axis=1)\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\nfrom keras.models import Model\nfrom keras.layers import Input, Dense\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\n\ndef build_autoencoder(input_dim, encoding_dim):\n    input_layer = Input(shape=(input_dim,))\n    \n    # Encoder: compressing the input\n    encoded = Dense(encoding_dim, activation='relu')(input_layer)\n    \n    # Decoder: reconstructing the input\n    decoded = Dense(input_dim, activation='sigmoid')(encoded)\n    \n    # Autoencoder model\n    autoencoder = Model(inputs=input_layer, outputs=decoded)\n    \n    # Encoder model (for getting the compressed representation)\n    encoder = Model(inputs=input_layer, outputs=encoded)\n    \n    autoencoder.compile(optimizer=Adam(), loss='mse')\n    \n    return autoencoder, encoder\n\ndef perform_autoencoder(df, encoding_dim=50, epochs=200, batch_size=32):\n    scaler = StandardScaler()\n    \n    # Exclude non-numeric columns\n    df_numeric = df.select_dtypes(include=[np.number])\n    \n    # Check for and handle NaN values\n    if df_numeric.isnull().values.any():\n        df_numeric.fillna(df_numeric.mean(), inplace=True)\n\n    if np.isinf(df_numeric).values.any():\n        df_numeric.replace([np.inf, -np.inf], np.nan, inplace=True)\n        df_numeric.fillna(df_numeric.mean(), inplace=True)\n\n    df_scaled = scaler.fit_transform(df_numeric)\n\n    # Check for NaN values after scaling\n    if np.isnan(df_scaled).any() or np.isinf(df_scaled).any():\n        print(\"NaN or infinite values detected in the scaled data!\")\n        return None  # or handle as needed\n\n    input_dim = df_scaled.shape[1]\n    autoencoder, encoder = build_autoencoder(input_dim, encoding_dim)\n    \n    # Use a smaller learning rate for stability\n    autoencoder.compile(optimizer=Adam(learning_rate=0.0001), loss='mse')\n    \n    # Set up early stopping\n    early_stopping = EarlyStopping(monitor='loss', patience=10, restore_best_weights=True)\n\n    # Train the autoencoder with early stopping\n    autoencoder.fit(df_scaled, df_scaled, \n                    epochs=epochs, \n                    batch_size=batch_size, \n                    shuffle=True, \n                    verbose=1, \n                    callbacks=[early_stopping])\n\n    encoded_data = encoder.predict(df_scaled)\n    \n    df_encoded = pd.DataFrame(encoded_data, columns=[f'Enc_{i+1}' for i in range(encoded_data.shape[1])])\n    \n    return df_encoded\n\n# # Perform autoencoder dimensionality reduction\n# train_ts_encoded = perform_autoencoder(df_train, encoding_dim=60, epochs=100, batch_size=32)\n# test_ts_encoded = perform_autoencoder(df_test, encoding_dim=60, epochs=100, batch_size=32)\n\ntrain_ts_encoded = perform_autoencoder(df_train, encoding_dim=64, epochs=256, batch_size=32)\ntest_ts_encoded = perform_autoencoder(df_test, encoding_dim=64,epochs=256, batch_size=32)\n\ntime_series_cols = train_ts_encoded.columns.tolist()\n#time_series_cols.remove(\"id\")\n\ntrain_ts_encoded[\"id\"]=train_ts[\"id\"]\n\ntest_ts_encoded['id']=test_ts[\"id\"]\n#test_ts_pca[\"id\"]=test_ts[\"id\"]\n\n#train = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts_encoded, how=\"left\", on='id')\n#test_ = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = pd.merge(train, train_ts_encoded, how=\"left\", on='id')\n#test = pd.merge(test, test_ts, how=\"inner\", on='id')\nanalysis=train\nanalysis = analysis.dropna(subset='Enc_4')\nanalysis=analysis.dropna(subset='sii')\nanalysis = analysis.drop('id', axis=1)\n\ndef engineer_features(df):\n    # Create interaction features\n    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n    \n    # Create categorical features\n    #df['Age_Group'] = pd.cut(df['Basic_Demos-Age'], bins=[0, 12, 18, 25, 100], labels=['Child', 'Teen', 'Young Adult', 'Adult'])\n    #df['BMI_Category'] = pd.cut(df['Physical-BMI'], bins=[0, 18.5, 25, 30, 100], labels=['Underweight', 'Normal', 'Overweight', 'Obese'])\n    \n    return df\n\ntest = engineer_features(test)\n\ntrain=engineer_features(train)\n\n#train = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\nnan_rows = train.query('sii.isna()', engine='python')\n\nimport pandas as pd\nfrom sklearn.impute import KNNImputer\n\n# Assuming 'train' is your DataFrame\n\n# Step 1: Create the KNN imputer\nimputer = KNNImputer(n_neighbors=5)  # You can adjust the number of neighbors\n\n# Step 2: Select numeric columns and fit the imputer\nnumeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\nimputed_data = imputer.fit_transform(train[numeric_cols])\n\n# Step 3: Create a new DataFrame with the imputed values\ntrain_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n\n# Step 4: Convert the 'sii' column back to integers\ntrain_imputed['sii'] = train_imputed['sii'].round().astype(int)\n\n# If there are other columns to retain, you can merge them back\nfor col in train.columns:\n    if col not in numeric_cols:\n        train_imputed[col] = train[col]\n\n# Now, check if 'sii' has been filled and is of integer type\nprint(train_imputed['sii'].isna().sum())  # Should be 0 if all NaNs were filled\nprint(train_imputed['sii'].dtype)  # Should show 'int'\n\ntrain_imputed['sii']=train['sii']\n\ntrain=train_imputed\n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii','BMI_Age','Internet_Hours_Age','BMI_Internet_Hours']\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\n#test= test[featuresCols]\ntrain = train.dropna(subset='sii')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', 'Fitness_Endurance-Season', \n          'FGC-Season', 'BIA-Season', 'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ndef update(df):\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday','BMI_Age','Internet_Hours_Age','BMI_Internet_Hours']\n\nfeaturesCols += time_series_cols\n\ntest= test[featuresCols]\n\ntrain = update(train)\ntest = update(test)\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\n\"\"\"This Mapping Works Fine For me I also Check Each Values in Train and test Using Logic. There no Data Lekage.\"\"\"\n\nfor col in cat_c:\n    mapping_train = create_mapping(col, train)\n    mapping_test = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping_train).astype(int)\n    test[col] = test[col].replace(mapping_test).astype(int)\n\nprint(f'Train Shape : {train.shape} || Test Shape : {test.shape}')\n\nanalysis=train.dropna(subset='Enc_51')\n\nmissing_df=train[train['Enc_51'].isna()]\n\nimport pandas as pd\n\n# Assuming analysis is your complete DataFrame (1000 data points)\n# and missing_df is your DataFrame with missing values (3000 data points)\n\n# Step 1: Store original data types\noriginal_dtypes = analysis.dtypes.to_dict()\n\n# Step 2: Split the complete data into features and target\nX_complete = analysis.drop('sii', axis=1)\ny_complete = analysis['sii']\n\n# Step 3: Identify categorical and numerical columns\ncategorical_features = [\n    'Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n    'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n    'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season'\n]\n\n# Step 3.1: Drop categorical columns for mean calculation\nnumerical_df = analysis.drop(columns=categorical_features + ['sii'])\n\n# Group by the target variable and calculate means for numerical features\nmean_impute = numerical_df.groupby(analysis['sii']).mean()\n\n# Group by the target variable and calculate modes for categorical features\nmode_impute = analysis[categorical_features].groupby(analysis['sii']).agg(\n    lambda x: x.mode()[0] if not x.mode().empty else None\n)\n\n# Step 4: Fill missing values class-wise\ndef fill_missing_values(row):\n    class_value = row['sii']\n    if pd.isnull(class_value):\n        return row  # No action if class_value is NaN\n\n    for col in row.index:\n        if pd.isnull(row[col]):\n            if col in mean_impute.columns and class_value in mean_impute.index:  # For numerical columns\n                row[col] = mean_impute.loc[class_value, col]\n            elif col in mode_impute.columns and class_value in mode_impute.index:  # For categorical columns\n                row[col] = mode_impute.loc[class_value, col]\n    return row\n\n# Apply the filling function to each row in the missing_df\ndf_missing_filled = missing_df.apply(fill_missing_values, axis=1)\n\n# Step 5: Combine datasets\nfinal_dataset = pd.concat([analysis, df_missing_filled], ignore_index=True)\n\n# Step 6: Convert categorical features to int64 safely\nfor col in categorical_features:\n    # Check for unique values and their counts\n    unique_values = final_dataset[col].unique()\n    print(f'Unique values in {col}: {unique_values}')\n\n    # Fill NaNs with a placeholder (optional)\n    final_dataset[col] = final_dataset[col].fillna('missing_value')\n\n    # Convert to category type first to manage the encoding\n    final_dataset[col] = final_dataset[col].astype('category')\n    \n    # Factorize the column\n    final_dataset[col] = pd.factorize(final_dataset[col])[0].astype('int64')\n\n# Step 7: Restore original data types for other columns\nfinal_dataset = final_dataset.astype(original_dtypes)\n\n# Now, final_dataset contains all your data with missing values filled and original data types restored\n\ntrain=final_dataset\n\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.ensemble import VotingRegressor\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.metrics import cohen_kappa_score\nfrom scipy.optimize import minimize\nfrom tqdm import tqdm\nfrom IPython.display import clear_output\n\n# Function definitions (unchanged)\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    # Ensure all categorical columns are encoded\n    for col in categorical_features:\n        X[col] = pd.factorize(X[col])[0].astype('int64')\n    \n    test_data = test_data.copy()\n    for col in categorical_features:\n        test_data[col] = pd.factorize(test_data[col])[0].astype('int64')\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {tKappa:.3f}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission\n\n# Model parameters and instantiation (unchanged)\nParams = {\n    'learning_rate': 0.046,\n    'max_depth': 12,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,\n    'lambda_l2': 0.01\n}\n\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,\n    'reg_lambda': 5,\n    'random_state': SEED\n}\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_state': SEED,\n    'cat_features': cat_c,\n    'verbose': 0,\n    'l2_leaf_reg': 10\n}\n\nLight = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n\n# Combine models using Voting Regressor\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n])\n\n# Train the ensemble model\nSubmission = TrainML(voting_model, test)\n\n# Save submission\n# Submission.to_csv('submission.csv', index=False)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-30T04:52:26.617955Z","iopub.execute_input":"2024-11-30T04:52:26.618457Z"}},"outputs":[{"name":"stderr","text":" 59%|█████▉    | 591/996 [04:18<02:09,  3.12it/s]","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nSEED = 42\nn_splits = 5\n\n# Load datasets\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\n\n\n\ndef process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\n\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    \n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    \n    stats, indexes = zip(*results)\n    \n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    return df\n\n        \ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\ntrain = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)   \n\n\n##########\n\n# import pandas as pd\n# from sklearn.impute import KNNImputer\n\n# # Assuming 'train' is your DataFrame\n\n# # Step 1: Create the KNN imputer\n# imputer = KNNImputer(n_neighbors=5)  # You can adjust the number of neighbors\n\n# # Step 2: Select numeric columns and fit the imputer\n# numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n# imputed_data = imputer.fit_transform(train[numeric_cols])\n\n# # Step 3: Create a new DataFrame with the imputed values\n# train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n\n# # Step 4: Convert the 'sii' column back to integers\n# train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n\n# # If there are other columns to retain, you can merge them back\n# for col in train.columns:\n#     if col not in numeric_cols:\n#         train_imputed[col] = train[col]\n\n# # Now, check if 'sii' has been filled and is of integer type\n# print(train_imputed['sii'].isna().sum())  # Should be 0 if all NaNs were filled\n# print(train_imputed['sii'].dtype)  # Should show 'int'\n\n# train_imputed['sii']=train['sii']\n\n# train=train_imputed\n\n\n#########\n\n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex',\n                'CGAS-Season', 'CGAS-CGAS_Score', 'Physical-Season', 'Physical-BMI',\n                'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP',\n                'Fitness_Endurance-Season', 'Fitness_Endurance-Max_Stage',\n                'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec',\n                'FGC-Season', 'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND',\n                'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone', 'FGC-FGC_PU',\n                'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR',\n                'FGC-FGC_SRR_Zone', 'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season',\n                'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM',\n                'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat', 'BIA-BIA_Frame_num',\n                'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM',\n                'BIA-BIA_TBW', 'PAQ_A-Season', 'PAQ_A-PAQ_A_Total', 'PAQ_C-Season',\n                'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw',\n                'SDS-SDS_Total_T', 'PreInt_EduHx-Season',\n                'PreInt_EduHx-computerinternet_hoursday', 'sii']\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n          'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n          'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n        \ntrain = update(train)\ntest = update(test)\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\nfor col in cat_c:\n    mapping = create_mapping(col, train)\n    mappingTe = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mappingTe).astype(int)\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission\n\n# Model parameters for LightGBM\nParams = {\n    'learning_rate': 0.046,\n    'max_depth': 12,\n    'num_leaves': 478,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,  # Increased from 6.59\n    'lambda_l2': 0.01  # Increased from 2.68e-06\n}\n\n\n# XGBoost parameters\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 6,\n    'n_estimators': 200,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,  # Increased from 0.1\n    'reg_lambda': 5,  # Increased from 1\n    'random_state': SEED\n}\n\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 6,\n    'iterations': 200,\n    'random_state': SEED,\n    'cat_features': cat_c,\n    'verbose': 0,\n    'l2_leaf_reg': 10  # Increase this value\n}\n\n# Create model instances\nLight = LGBMRegressor(**Params, random_state=SEED, verbose=-1, n_estimators=300)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n\n# Combine models using Voting Regressor\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n])\n\n# Train the ensemble model\nSubmission1 = TrainML(voting_model, test)\n\n# Save submission\n#Submission1.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport re\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom concurrent.futures import ThreadPoolExecutor\nfrom tqdm import tqdm\n\nfrom colorama import Fore, Style\nfrom IPython.display import clear_output\nimport warnings\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor, RandomForestRegressor, GradientBoostingRegressor\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\nSEED = 42\nn_splits = 5\n\n# Load datasets\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\ndef process_file(filename, dirname):\n    df = pd.read_parquet(os.path.join(dirname, filename, 'part-0.parquet'))\n    df.drop('step', axis=1, inplace=True)\n    return df.describe().values.reshape(-1), filename.split('=')[1]\n\ndef load_time_series(dirname) -> pd.DataFrame:\n    ids = os.listdir(dirname)\n    with ThreadPoolExecutor() as executor:\n        results = list(tqdm(executor.map(lambda fname: process_file(fname, dirname), ids), total=len(ids)))\n    stats, indexes = zip(*results)\n    df = pd.DataFrame(stats, columns=[f\"stat_{i}\" for i in range(len(stats[0]))])\n    df['id'] = indexes\n    return df\n\ntrain_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_train.parquet\")\ntest_ts = load_time_series(\"/kaggle/input/child-mind-institute-problematic-internet-use/series_test.parquet\")\n\ntime_series_cols = train_ts.columns.tolist()\ntime_series_cols.remove(\"id\")\n\ntrain = pd.merge(train, train_ts, how=\"left\", on='id')\ntest = pd.merge(test, test_ts, how=\"left\", on='id')\n\ntrain = train.drop('id', axis=1)\ntest = test.drop('id', axis=1)\n\n\n#####\n\n\n# import pandas as pd\n# from sklearn.impute import KNNImputer\n\n# # Assuming 'train' is your DataFrame\n\n# # Step 1: Create the KNN imputer\n# imputer = KNNImputer(n_neighbors=5)  # You can adjust the number of neighbors\n\n# # Step 2: Select numeric columns and fit the imputer\n# numeric_cols = train.select_dtypes(include=['float64', 'int64']).columns\n# imputed_data = imputer.fit_transform(train[numeric_cols])\n\n# # Step 3: Create a new DataFrame with the imputed values\n# train_imputed = pd.DataFrame(imputed_data, columns=numeric_cols)\n\n# # Step 4: Convert the 'sii' column back to integers\n# train_imputed['sii'] = train_imputed['sii'].round().astype(int)\n\n# # If there are other columns to retain, you can merge them back\n# for col in train.columns:\n#     if col not in numeric_cols:\n#         train_imputed[col] = train[col]\n\n# # Now, check if 'sii' has been filled and is of integer type\n# print(train_imputed['sii'].isna().sum())  # Should be 0 if all NaNs were filled\n# print(train_imputed['sii'].dtype)  # Should show 'int'\n\n# train_imputed['sii']=train['sii']\n\n# train=train_imputed\n\n#####\n\nfeaturesCols = ['Basic_Demos-Enroll_Season', 'Basic_Demos-Age', 'Basic_Demos-Sex', 'CGAS-Season', 'CGAS-CGAS_Score',\n                'Physical-Season', 'Physical-BMI', 'Physical-Height', 'Physical-Weight', 'Physical-Waist_Circumference',\n                'Physical-Diastolic_BP', 'Physical-HeartRate', 'Physical-Systolic_BP', 'Fitness_Endurance-Season',\n                'Fitness_Endurance-Max_Stage', 'Fitness_Endurance-Time_Mins', 'Fitness_Endurance-Time_Sec', 'FGC-Season',\n                'FGC-FGC_CU', 'FGC-FGC_CU_Zone', 'FGC-FGC_GSND', 'FGC-FGC_GSND_Zone', 'FGC-FGC_GSD', 'FGC-FGC_GSD_Zone',\n                'FGC-FGC_PU', 'FGC-FGC_PU_Zone', 'FGC-FGC_SRL', 'FGC-FGC_SRL_Zone', 'FGC-FGC_SRR', 'FGC-FGC_SRR_Zone',\n                'FGC-FGC_TL', 'FGC-FGC_TL_Zone', 'BIA-Season', 'BIA-BIA_Activity_Level_num', 'BIA-BIA_BMC', 'BIA-BIA_BMI',\n                'BIA-BIA_BMR', 'BIA-BIA_DEE', 'BIA-BIA_ECW', 'BIA-BIA_FFM', 'BIA-BIA_FFMI', 'BIA-BIA_FMI', 'BIA-BIA_Fat',\n                'BIA-BIA_Frame_num', 'BIA-BIA_ICW', 'BIA-BIA_LDM', 'BIA-BIA_LST', 'BIA-BIA_SMM', 'BIA-BIA_TBW', 'PAQ_A-Season',\n                'PAQ_A-PAQ_A_Total', 'PAQ_C-Season', 'PAQ_C-PAQ_C_Total', 'SDS-Season', 'SDS-SDS_Total_Raw', 'SDS-SDS_Total_T',\n                'PreInt_EduHx-Season', 'PreInt_EduHx-computerinternet_hoursday', 'sii']\n\nfeaturesCols += time_series_cols\n\ntrain = train[featuresCols]\ntrain = train.dropna(subset='sii')\n\ncat_c = ['Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', 'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n         'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season']\n\ndef update(df):\n    global cat_c\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n\ntrain = update(train)\ntest = update(test)\n\ndef create_mapping(column, dataset):\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\nfor col in cat_c:\n    mapping = create_mapping(col, train)\n    mappingTe = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping).astype(int)\n    test[col] = test[col].replace(mappingTe).astype(int)\n\ndef quadratic_weighted_kappa(y_true, y_pred):\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {Fore.CYAN}{Style.BRIGHT} {tKappa:.3f}{Style.RESET_ALL}\")\n\n    tpm = test_preds.mean(axis=1)\n    tp_rounded = threshold_Rounder(tpm, KappaOPtimizer.x)\n\n    return tp_rounded\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\n\n# Imputation step: Filling missing values with the median\nimputer = SimpleImputer(strategy='median')\n\n# Updating the ensemble to include the RandomForest and GradientBoosting models\nensemble = VotingRegressor(estimators=[\n    ('lgb', Pipeline(steps=[('imputer', imputer), ('regressor', LGBMRegressor(random_state=SEED))])),\n    ('xgb', Pipeline(steps=[('imputer', imputer), ('regressor', XGBRegressor(random_state=SEED))])),\n    ('cat', Pipeline(steps=[('imputer', imputer), ('regressor', CatBoostRegressor(random_state=SEED, silent=True))])),\n    ('rf', Pipeline(steps=[('imputer', imputer), ('regressor', RandomForestRegressor(random_state=SEED))])),\n    ('gb', Pipeline(steps=[('imputer', imputer), ('regressor', GradientBoostingRegressor(random_state=SEED))]))\n])\n\n# Train the ensemble with the updated model pipeline\npredictions = TrainML(ensemble, test)\n\n# Save predictions to a CSV file\nsample['sii'] = predictions\n#sample.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================\n# Import Necessary Libraries\n# ==============================================\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport warnings\n\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom scipy.optimize import minimize\nfrom sklearn.impute import KNNImputer\n\nfrom scipy.stats import mode  # For majority voting\n\n# PyTorch Libraries\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\n# Suppress warnings and set display options\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\n# Set random seed for reproducibility\nSEED = 42\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n\n# ==============================================\n# Load and Merge Data\n# ==============================================\n# Load main datasets\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\n\n# ==============================================\n# Feature Engineering\n# ==============================================\ndef engineer_features(df):\n    \"\"\"\n    Create interaction features.\n    \"\"\"\n    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['Pulse_Pressure'] = df['Physical-Systolic_BP'] - df['Physical-Diastolic_BP']\n    df['HeartRate_Age'] = df['Physical-HeartRate'] * df['Basic_Demos-Age']\n    df['Fitness_Score'] = df['Fitness_Endurance-Max_Stage'] * (\n        df['Fitness_Endurance-Time_Mins'] * 60 + df['Fitness_Endurance-Time_Sec']\n    )\n    df['FMI_FFMI_Ratio'] = df['BIA-BIA_FMI'] / (df['BIA-BIA_FFMI'] + 1e-6)\n    df['Sleep_Internet_Hours'] = df['SDS-SDS_Total_T'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['Waist_Height_Ratio'] = df['Physical-Waist_Circumference'] / (df['Physical-Height'] + 1e-6)\n    return df\n\n# Apply feature engineering to both train and test datasets\ntrain = engineer_features(train)\ntest = engineer_features(test)\n\n# ==============================================\n# Define Feature Columns\n# ==============================================\nfeaturesCols = list(set(train.columns).intersection(set(test.columns)))\n# featuresCols = [\n#     'Internet_Hours_Age', 'Physical-Height', 'Sleep_Internet_Hours', 'Basic_Demos-Age', \n#     'BMI_Internet_Hours', 'HeartRate_Age', 'BMI_Age', 'Physical-Waist_Circumference', \n#     'FGC-FGC_CU', 'BIA-BIA_BMI', 'SDS-SDS_Total_Raw', 'FGC-FGC_GSND', 'Physical-BMI', \n#     'FGC-FGC_GSD', 'FGC-FGC_PU', 'BIA-BIA_FFMI', 'BIA-BIA_Frame_num', 'Physical-Systolic_BP', \n#     'Pulse_Pressure', 'FGC-FGC_TL', 'Basic_Demos-Sex', 'FGC-FGC_SRL_Zone', 'CGAS-CGAS_Score', \n#     'Fitness_Score', 'BIA-BIA_SMM', 'BIA-BIA_Activity_Level_num'\n# ]\n\n# Select the features from train and test datasets\ntrain_df = train[featuresCols + ['sii']]  # Include 'sii' in train features\ntest_df = test[featuresCols]\n\n# Drop 'id' from train and test data if present\ntrain_df = train_df.drop('id', axis=1, errors='ignore')\ntest_df = test_df.drop('id', axis=1, errors='ignore')\n\n# Drop rows with missing target variable 'sii' in train data\ntrain_df = train_df.dropna(subset=['sii'])\n\nfeaturesCols = list(test_df.columns)\n\n# ==============================================\n# Handle Categorical Variables\n# ==============================================\n# List of categorical columns\ncat_c = [\n    'Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n    'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n    'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season'\n]\n\ndef create_consistent_mapping(column):\n    \"\"\"\n    Create a consistent mapping for categorical variables by combining unique values from both datasets.\n    \"\"\"\n    combined_values = pd.concat([train_df[column], test_df[column]], axis=0).fillna('Missing')\n    unique_values = combined_values.unique()\n    sorted_values = np.sort(unique_values)  # Ensure consistent ordering\n    return {value: idx for idx, value in enumerate(sorted_values)}\n\n# Map categorical variables to integers using a consistent mapping\nfor col in cat_c:\n    mapping = create_consistent_mapping(col)\n    train_df[col] = train_df[col].fillna('Missing').map(mapping).astype(int)\n    test_df[col] = test_df[col].fillna('Missing').map(mapping).astype(int)\n\n\n# ==============================================\n# Handle Missing Values with KNN Imputer\n# ==============================================\n# Impute missing values using KNNImputer\nimputer = KNNImputer(n_neighbors=3)\nnumeric_cols = train_df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n\n# Remove 'sii' from numeric_cols when applying to test data\nif 'sii' in numeric_cols:\n    numeric_cols.remove('sii')\n\n# Impute train data\ntrain_df[numeric_cols] = imputer.fit_transform(train_df[numeric_cols])\n\n# Impute test data\n# Ensure that all columns in numeric_cols exist in test data\nnumeric_cols_test = [col for col in numeric_cols if col in test_df.columns]\ntest_df[numeric_cols_test] = imputer.transform(test_df[numeric_cols_test])\n\n# Ensure 'sii' remains integer\ntrain_df['sii'] = train_df['sii'].round().astype(int)\n\n# ==============================================\n# Model Training and Evaluation Functions\n# ==============================================\ndef quadratic_weighted_kappa(y_true, y_pred):\n    \"\"\"\n    Calculate Quadratic Weighted Kappa.\n    \"\"\"\n    y_true = y_true.astype(int)\n    y_pred = y_pred.astype(int)\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    \"\"\"\n    Apply thresholds to continuous predictions.\n    \"\"\"\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    \"\"\"\n    Objective function for optimizing thresholds.\n    \"\"\"\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\n# ==============================================\n# Split Features Among Expert Networks\n# ==============================================\nfeature_splits = np.array_split(featuresCols, 4)\nfeatures_expert1 = feature_splits[0].tolist()\nfeatures_expert2 = feature_splits[1].tolist()\nfeatures_expert3 = feature_splits[2].tolist()\nfeatures_expert4 = feature_splits[3].tolist()\n\n# ==============================================\n# Prepare Data for Training\n# ==============================================\n# Inputs for expert networks\nX_expert1 = train_df[features_expert1]\nX_expert2 = train_df[features_expert2]\nX_expert3 = train_df[features_expert3]\nX_expert4 = train_df[features_expert4]\n\n# Input for weighting network\nX_weighting = train_df[featuresCols]  # All features\n\n# Target variable\ny = train_df['sii']\n\n# Stratified Split\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nsss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\ntrain_index, val_index = next(sss.split(X_weighting, y))\n\nX_expert_list_train = [X.iloc[train_index].reset_index(drop=True) for X in [X_expert1, X_expert2, X_expert3, X_expert4]]\nX_weighting_train = X_weighting.iloc[train_index].reset_index(drop=True)\ny_train = y.iloc[train_index].reset_index(drop=True)\n\nX_expert_list_val = [X.iloc[val_index].reset_index(drop=True) for X in [X_expert1, X_expert2, X_expert3, X_expert4]]\nX_weighting_val = X_weighting.iloc[val_index].reset_index(drop=True)\ny_val = y.iloc[val_index].reset_index(drop=True)\n\n# ==============================================\n# Define Custom Dataset Class\n# ==============================================\nclass MultiInputDataset(Dataset):\n    def __init__(self, X_expert_list, X_weighting, y=None):\n        self.X_expert_list = [torch.tensor(X.values, dtype=torch.float32) for X in X_expert_list]\n        self.X_weighting = torch.tensor(X_weighting.values, dtype=torch.float32)\n        if y is not None:\n            self.y = torch.tensor(y.values, dtype=torch.float32)\n        else:\n            self.y = None\n    def __len__(self):\n        return len(self.X_weighting)\n    def __getitem__(self, idx):\n        inputs = [X[idx] for X in self.X_expert_list]\n        weighting_input = self.X_weighting[idx]\n        if self.y is not None:\n            return inputs, weighting_input, self.y[idx]\n        else:\n            return inputs, weighting_input\n\n# Create datasets and data loaders\ntrain_dataset = MultiInputDataset(X_expert_list_train, X_weighting_train, y_train)\nval_dataset = MultiInputDataset(X_expert_list_val, X_weighting_val, y_val)\n\nbatch_size = 64\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# Prepare test data\nX_expert1_test = test_df[features_expert1]\nX_expert2_test = test_df[features_expert2]\nX_expert3_test = test_df[features_expert3]\nX_expert4_test = test_df[features_expert4]\nX_weighting_test = test_df[featuresCols]\n\ntest_dataset = MultiInputDataset(\n    [X_expert1_test.reset_index(drop=True), X_expert2_test.reset_index(drop=True), \n     X_expert3_test.reset_index(drop=True), X_expert4_test.reset_index(drop=True)],\n    X_weighting_test.reset_index(drop=True)\n)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# ==============================================\n# Define Neural Network Architectures\n# ==============================================\nclass ExpertNet(nn.Module):\n    def __init__(self, input_size):\n        super(ExpertNet, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 64),\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.BatchNorm1d(32),\n            nn.Linear(32, 1)\n        )\n    def forward(self, x):\n        return self.fc(x).squeeze()  # Output shape: [batch_size]\n\nclass WeightingNet(nn.Module):\n    def __init__(self, input_size):\n        super(WeightingNet, self).__init__()\n        self.fc = nn.Sequential(\n            nn.Linear(input_size, 64),\n            nn.ReLU(),\n            nn.BatchNorm1d(64),\n            nn.Linear(64, 32),\n            nn.ReLU(),\n            nn.BatchNorm1d(32),\n            nn.Linear(32, 4),\n            nn.Softmax(dim=1)  # Output weights that sum to 1\n        )\n    def forward(self, x):\n        return self.fc(x)  # Output shape: [batch_size, 4]\n\n# Initialize networks\ninput_sizes = [len(features_expert1), len(features_expert2), len(features_expert3), len(features_expert4)]\nexpert_nets = [ExpertNet(input_size) for input_size in input_sizes]\nweighting_net = WeightingNet(len(featuresCols))\n\n# Move models to GPU if available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nfor net in expert_nets:\n    net.to(device)\nweighting_net.to(device)\n\n# ==============================================\n# Define Loss Function and Optimizer\n# ==============================================\nclass OrdinalLoss(nn.Module):\n    def __init__(self):\n        super(OrdinalLoss, self).__init__()\n    def forward(self, preds, targets):\n        # preds: continuous predictions [batch_size]\n        # targets: true labels [batch_size]\n        # Compute loss that penalizes deviations more severely for larger errors\n        loss = torch.mean((preds - targets.float()) ** 2)\n        return loss\n\ncriterion = OrdinalLoss()\n\noptimizer = optim.AdamW(\n    list(expert_nets[0].parameters()) +\n    list(expert_nets[1].parameters()) +\n    list(expert_nets[2].parameters()) +\n    list(expert_nets[3].parameters()) +\n    list(weighting_net.parameters())\n)\n\n# ==============================================\n# Training Loop\n# ==============================================\nnum_epochs = 350\n\nfor epoch in range(num_epochs):\n    expert_nets[0].train()\n    expert_nets[1].train()\n    expert_nets[2].train()\n    expert_nets[3].train()\n    weighting_net.train()\n    \n    running_loss = 0.0\n    for inputs_list, weighting_input, labels in train_loader:\n        optimizer.zero_grad()\n        \n        # Move data to device\n        inputs_list = [inp.to(device) for inp in inputs_list]\n        weighting_input = weighting_input.to(device)\n        labels = labels.to(device)\n        \n        # Get outputs from expert networks\n        expert_outputs = []\n        for i in range(4):\n            expert_outputs.append(expert_nets[i](inputs_list[i]))\n        expert_outputs = torch.stack(expert_outputs, dim=1)  # Shape: [batch_size, 4]\n        \n        # Get weights from weighting network\n        weights = weighting_net(weighting_input)  # Shape: [batch_size, 4]\n        \n        # Compute weighted sum\n        preds = (expert_outputs * weights).sum(dim=1)  # Shape: [batch_size]\n        \n        # Compute loss\n        loss = criterion(preds, labels)\n        \n        # Backpropagate\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item() * labels.size(0)\n    epoch_loss = running_loss / len(train_dataset)\n    \n    # Validation\n    expert_nets[0].eval()\n    expert_nets[1].eval()\n    expert_nets[2].eval()\n    expert_nets[3].eval()\n    weighting_net.eval()\n    val_preds = []\n    val_labels = []\n    with torch.no_grad():\n        for inputs_list, weighting_input, labels in val_loader:\n            # Move data to device\n            inputs_list = [inp.to(device) for inp in inputs_list]\n            weighting_input = weighting_input.to(device)\n            labels = labels.to(device)\n            \n            # Get outputs from expert networks\n            expert_outputs = []\n            for i in range(4):\n                expert_outputs.append(expert_nets[i](inputs_list[i]))\n            expert_outputs = torch.stack(expert_outputs, dim=1)  # Shape: [batch_size, 4]\n            \n            # Get weights from weighting network\n            weights = weighting_net(weighting_input)  # Shape: [batch_size, 4]\n            \n            # Compute weighted sum\n            preds = (expert_outputs * weights).sum(dim=1)  # Shape: [batch_size]\n            \n            val_preds.append(preds.cpu().numpy())\n            val_labels.append(labels.cpu().numpy())\n    val_preds = np.concatenate(val_preds)\n    val_labels = np.concatenate(val_labels)\n    \n    # Compute validation loss and QWK\n    val_loss = np.mean((val_preds - val_labels) ** 2)\n    val_preds_rounded = np.clip(np.round(val_preds), 0, 3).astype(int)\n    val_kappa = quadratic_weighted_kappa(val_labels, val_preds_rounded)\n    if val_kappa > 0.38:\n        break\n    \n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, Val Loss: {val_loss:.4f}, Val QWK: {val_kappa:.4f}\")\n\n# ==============================================\n# Optimize Thresholds\n# ==============================================\nfrom scipy.optimize import minimize\n\n# Optimize thresholds\nKappaOptimizer = minimize(evaluate_predictions,\n                          x0=[0.5, 1.5, 2.5], args=(val_labels, val_preds), \n                          method='Nelder-Mead')\nbest_thresholds = KappaOptimizer.x\n\n# Evaluate optimized QWK\nval_preds_tuned = threshold_Rounder(val_preds, best_thresholds)\nval_kappa_tuned = quadratic_weighted_kappa(val_labels, val_preds_tuned)\nprint(f\"Optimized Validation QWK: {val_kappa_tuned:.4f}\")\n\n# ==============================================\n# Generate Predictions on Test Data\n# ==============================================\nexpert_nets[0].eval()\nexpert_nets[1].eval()\nexpert_nets[2].eval()\nexpert_nets[3].eval()\nweighting_net.eval()\n\ntest_preds = []\nwith torch.no_grad():\n    for inputs_list, weighting_input in test_loader:\n        # Move data to device\n        inputs_list = [inp.to(device) for inp in inputs_list]\n        weighting_input = weighting_input.to(device)\n        \n        # Get outputs from expert networks\n        expert_outputs = []\n        for i in range(4):\n            expert_outputs.append(expert_nets[i](inputs_list[i]))\n        expert_outputs = torch.stack(expert_outputs, dim=1)  # Shape: [batch_size, 4]\n        \n        # Get weights from weighting network\n        weights = weighting_net(weighting_input)  # Shape: [batch_size, 4]\n        \n        # Compute weighted sum\n        preds = (expert_outputs * weights).sum(dim=1)  # Shape: [batch_size]\n        \n        test_preds.append(preds.cpu().numpy())\ntest_preds = np.concatenate(test_preds)\n\n# Apply optimized thresholds\ntest_preds_tuned = threshold_Rounder(test_preds, best_thresholds)\ntest_preds_tuned = test_preds_tuned.astype(int)\n\n# Create submission DataFrame\nsubmission4 = pd.DataFrame({\n    'id': test['id'],\n    'sii': test_preds_tuned\n})\n\n# Save submission\n# submission4.to_csv('submission.csv', index=False)\n# print(\"Submission file 'submission.csv' has been created.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-01T17:05:29.768017Z","iopub.execute_input":"2024-12-01T17:05:29.768521Z","iopub.status.idle":"2024-12-01T17:06:11.697181Z","shell.execute_reply.started":"2024-12-01T17:05:29.768486Z","shell.execute_reply":"2024-12-01T17:06:11.696013Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/350, Loss: 0.6882, Val Loss: 0.6590, Val QWK: 0.1572\nEpoch 2/350, Loss: 0.5102, Val Loss: 0.5615, Val QWK: 0.2534\nEpoch 3/350, Loss: 0.4958, Val Loss: 0.5065, Val QWK: 0.2922\nEpoch 4/350, Loss: 0.4859, Val Loss: 0.5144, Val QWK: 0.2959\nEpoch 5/350, Loss: 0.4651, Val Loss: 0.5259, Val QWK: 0.2486\nEpoch 6/350, Loss: 0.4705, Val Loss: 0.5472, Val QWK: 0.2881\nEpoch 7/350, Loss: 0.4625, Val Loss: 0.5675, Val QWK: 0.2982\nEpoch 8/350, Loss: 0.4542, Val Loss: 0.5168, Val QWK: 0.2884\nEpoch 9/350, Loss: 0.4580, Val Loss: 0.5178, Val QWK: 0.3134\nEpoch 10/350, Loss: 0.4506, Val Loss: 0.5047, Val QWK: 0.3279\nEpoch 11/350, Loss: 0.4417, Val Loss: 0.4729, Val QWK: 0.3401\nEpoch 12/350, Loss: 0.4359, Val Loss: 0.5015, Val QWK: 0.3059\nEpoch 13/350, Loss: 0.4387, Val Loss: 0.4895, Val QWK: 0.3162\nEpoch 14/350, Loss: 0.4365, Val Loss: 0.5335, Val QWK: 0.2994\nEpoch 15/350, Loss: 0.4337, Val Loss: 0.5220, Val QWK: 0.3168\nEpoch 16/350, Loss: 0.4393, Val Loss: 0.5056, Val QWK: 0.2937\nEpoch 17/350, Loss: 0.4336, Val Loss: 0.5020, Val QWK: 0.3392\nEpoch 18/350, Loss: 0.4281, Val Loss: 0.4884, Val QWK: 0.3317\nEpoch 19/350, Loss: 0.4215, Val Loss: 0.4899, Val QWK: 0.3294\nEpoch 20/350, Loss: 0.4146, Val Loss: 0.4840, Val QWK: 0.3738\nEpoch 21/350, Loss: 0.4261, Val Loss: 0.5212, Val QWK: 0.2940\nEpoch 22/350, Loss: 0.4203, Val Loss: 0.4949, Val QWK: 0.3288\nEpoch 23/350, Loss: 0.4229, Val Loss: 0.4829, Val QWK: 0.3502\nEpoch 24/350, Loss: 0.4132, Val Loss: 0.4893, Val QWK: 0.3593\nEpoch 25/350, Loss: 0.4048, Val Loss: 0.5014, Val QWK: 0.2917\nEpoch 26/350, Loss: 0.4046, Val Loss: 0.5305, Val QWK: 0.3312\nEpoch 27/350, Loss: 0.4048, Val Loss: 0.5357, Val QWK: 0.3224\nEpoch 28/350, Loss: 0.4075, Val Loss: 0.5314, Val QWK: 0.3092\nEpoch 29/350, Loss: 0.4160, Val Loss: 0.5061, Val QWK: 0.3382\nEpoch 30/350, Loss: 0.4096, Val Loss: 0.6401, Val QWK: 0.3294\nEpoch 31/350, Loss: 0.4091, Val Loss: 0.4774, Val QWK: 0.3506\nEpoch 32/350, Loss: 0.4051, Val Loss: 0.5069, Val QWK: 0.3303\nEpoch 33/350, Loss: 0.4049, Val Loss: 0.4825, Val QWK: 0.3350\nEpoch 34/350, Loss: 0.3965, Val Loss: 0.5097, Val QWK: 0.3045\nEpoch 35/350, Loss: 0.4063, Val Loss: 0.4779, Val QWK: 0.3555\nEpoch 36/350, Loss: 0.4029, Val Loss: 0.4891, Val QWK: 0.3233\nEpoch 37/350, Loss: 0.4004, Val Loss: 0.5103, Val QWK: 0.2641\nEpoch 38/350, Loss: 0.4041, Val Loss: 0.5312, Val QWK: 0.2361\nEpoch 39/350, Loss: 0.3944, Val Loss: 0.5475, Val QWK: 0.2957\nEpoch 40/350, Loss: 0.3968, Val Loss: 0.4948, Val QWK: 0.3492\nEpoch 41/350, Loss: 0.3978, Val Loss: 0.4949, Val QWK: 0.2909\nEpoch 42/350, Loss: 0.3963, Val Loss: 0.4928, Val QWK: 0.3496\nEpoch 43/350, Loss: 0.3836, Val Loss: 0.5112, Val QWK: 0.2982\nEpoch 44/350, Loss: 0.3815, Val Loss: 0.5227, Val QWK: 0.3135\nEpoch 45/350, Loss: 0.3914, Val Loss: 0.5298, Val QWK: 0.2863\nEpoch 46/350, Loss: 0.3825, Val Loss: 0.5021, Val QWK: 0.3195\nEpoch 47/350, Loss: 0.3813, Val Loss: 0.5055, Val QWK: 0.2985\nEpoch 48/350, Loss: 0.3790, Val Loss: 0.5191, Val QWK: 0.3619\nEpoch 49/350, Loss: 0.3792, Val Loss: 0.4979, Val QWK: 0.2991\nEpoch 50/350, Loss: 0.3870, Val Loss: 0.5309, Val QWK: 0.3193\nEpoch 51/350, Loss: 0.3750, Val Loss: 0.4762, Val QWK: 0.3707\nEpoch 52/350, Loss: 0.3792, Val Loss: 0.5221, Val QWK: 0.2887\nEpoch 53/350, Loss: 0.3702, Val Loss: 0.5306, Val QWK: 0.3286\nEpoch 54/350, Loss: 0.3724, Val Loss: 0.5654, Val QWK: 0.3308\nEpoch 55/350, Loss: 0.3649, Val Loss: 0.5268, Val QWK: 0.2942\nEpoch 56/350, Loss: 0.3633, Val Loss: 0.5538, Val QWK: 0.3391\nEpoch 57/350, Loss: 0.3678, Val Loss: 0.5438, Val QWK: 0.3031\nEpoch 58/350, Loss: 0.3615, Val Loss: 0.5499, Val QWK: 0.3297\nEpoch 59/350, Loss: 0.3575, Val Loss: 0.5513, Val QWK: 0.3058\nEpoch 60/350, Loss: 0.3571, Val Loss: 0.5650, Val QWK: 0.3341\nEpoch 61/350, Loss: 0.3617, Val Loss: 0.5207, Val QWK: 0.3116\nEpoch 62/350, Loss: 0.3638, Val Loss: 0.5544, Val QWK: 0.3145\nEpoch 63/350, Loss: 0.3529, Val Loss: 0.5227, Val QWK: 0.3364\nEpoch 64/350, Loss: 0.3512, Val Loss: 0.5065, Val QWK: 0.3592\nEpoch 65/350, Loss: 0.3709, Val Loss: 0.5363, Val QWK: 0.3637\nEpoch 66/350, Loss: 0.3551, Val Loss: 0.5269, Val QWK: 0.3252\nEpoch 67/350, Loss: 0.3574, Val Loss: 0.6043, Val QWK: 0.2853\nEpoch 68/350, Loss: 0.3584, Val Loss: 0.5267, Val QWK: 0.3212\nEpoch 69/350, Loss: 0.3513, Val Loss: 0.5714, Val QWK: 0.2383\nEpoch 70/350, Loss: 0.3545, Val Loss: 0.5493, Val QWK: 0.2743\nEpoch 71/350, Loss: 0.3481, Val Loss: 0.5475, Val QWK: 0.2663\nEpoch 72/350, Loss: 0.3550, Val Loss: 0.5525, Val QWK: 0.3326\nEpoch 73/350, Loss: 0.3472, Val Loss: 0.6146, Val QWK: 0.2665\nEpoch 74/350, Loss: 0.3476, Val Loss: 0.5660, Val QWK: 0.3321\nEpoch 75/350, Loss: 0.3460, Val Loss: 0.5127, Val QWK: 0.3621\nEpoch 76/350, Loss: 0.3517, Val Loss: 0.5290, Val QWK: 0.3386\nEpoch 77/350, Loss: 0.3452, Val Loss: 0.5455, Val QWK: 0.3414\nEpoch 78/350, Loss: 0.3488, Val Loss: 0.5783, Val QWK: 0.3484\nEpoch 79/350, Loss: 0.3536, Val Loss: 0.5440, Val QWK: 0.3498\nEpoch 80/350, Loss: 0.3469, Val Loss: 0.5340, Val QWK: 0.3473\nEpoch 81/350, Loss: 0.3401, Val Loss: 0.5345, Val QWK: 0.3152\nEpoch 82/350, Loss: 0.3323, Val Loss: 0.5919, Val QWK: 0.3442\nEpoch 83/350, Loss: 0.3354, Val Loss: 0.5747, Val QWK: 0.3409\nEpoch 84/350, Loss: 0.3251, Val Loss: 0.5579, Val QWK: 0.3489\nEpoch 85/350, Loss: 0.3315, Val Loss: 0.5814, Val QWK: 0.2822\nEpoch 86/350, Loss: 0.3643, Val Loss: 0.5659, Val QWK: 0.3711\nEpoch 87/350, Loss: 0.3422, Val Loss: 0.5530, Val QWK: 0.3428\nEpoch 88/350, Loss: 0.3346, Val Loss: 0.5692, Val QWK: 0.2975\nEpoch 89/350, Loss: 0.3247, Val Loss: 0.5709, Val QWK: 0.3573\nEpoch 90/350, Loss: 0.3304, Val Loss: 0.5840, Val QWK: 0.2821\nEpoch 91/350, Loss: 0.3331, Val Loss: 0.5585, Val QWK: 0.3190\nEpoch 92/350, Loss: 0.3324, Val Loss: 0.5708, Val QWK: 0.3466\nEpoch 93/350, Loss: 0.3282, Val Loss: 0.5468, Val QWK: 0.3246\nOptimized Validation QWK: 0.4062\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ==============================================\n# Import Necessary Libraries\n# ==============================================\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom IPython.display import clear_output\nimport warnings\n\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom sklearn.impute import KNNImputer\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor\n\n# Suppress warnings and set display options\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\n# Set random seed and number of folds for cross-validation\nSEED = 42\nn_splits = 10\n\n# ==============================================\n# Load and Merge Data\n# ==============================================\n# Load main datasets\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\n# ==============================================\n# Feature Engineering\n# ==============================================\ndef engineer_features(df):\n    \"\"\"\n    Create interaction features.\n    \"\"\"\n    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['Pulse_Pressure'] = df['Physical-Systolic_BP'] - df['Physical-Diastolic_BP']\n    df['HeartRate_Age'] = df['Physical-HeartRate'] * df['Basic_Demos-Age']\n    df['Fitness_Score'] = df['Fitness_Endurance-Max_Stage'] * (\n        df['Fitness_Endurance-Time_Mins'] * 60 + df['Fitness_Endurance-Time_Sec']\n    )\n    df['FMI_FFMI_Ratio'] = df['BIA-BIA_FMI'] / (df['BIA-BIA_FFMI'] + 1e-6)\n    df['Sleep_Internet_Hours'] = df['SDS-SDS_Total_T'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['Waist_Height_Ratio'] = df['Physical-Waist_Circumference'] / (df['Physical-Height'] + 1e-6)\n    return df\n\n# Apply feature engineering to both train and test datasets\ntrain = engineer_features(train)\ntest = engineer_features(test)\n\n# ==============================================\n# Define Feature Columns\n# ==============================================\n# Define featuresCols as the common columns between train and test\nfeaturesCols = list(set(train.columns).intersection(set(test.columns)))\n\n# Select the features from train and test datasets\ntrain = train[featuresCols + ['sii']]  # Include 'sii' in train features\ntest = test[featuresCols]\n\n# Drop 'id' from train and test data if present\ntrain = train.drop('id', axis=1, errors='ignore')\ntest = test.drop('id', axis=1, errors='ignore')\n\n# Drop rows with missing target variable 'sii' in train data\ntrain = train.dropna(subset=['sii'])\n\n# ==============================================\n# Handle Categorical Variables\n# ==============================================\n# List of categorical columns\ncat_c = [\n    'Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n    'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n    'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season'\n]\n\ndef update_categorical(df):\n    \"\"\"\n    Fill missing values and convert columns to categorical.\n    \"\"\"\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n\n# Update categorical columns in both datasets\ntrain = update_categorical(train)\ntest = update_categorical(test)\n\ndef create_mapping(column, dataset):\n    \"\"\"\n    Create a mapping for categorical variables.\n    \"\"\"\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\n# Map categorical variables to integers\nfor col in cat_c:\n    mapping_train = create_mapping(col, train)\n    mapping_test = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping_train).astype(int)\n    test[col] = test[col].replace(mapping_test).astype(int)\n\n# ==============================================\n# Handle Missing Values with KNN Imputer\n# ==============================================\n# Impute missing values using KNNImputer\nimputer = KNNImputer(n_neighbors=5)\nnumeric_cols = train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n\n# Remove 'sii' from numeric_cols when applying to test data\nif 'sii' in numeric_cols:\n    numeric_cols.remove('sii')\n\n# Impute train data\ntrain[numeric_cols] = imputer.fit_transform(train[numeric_cols])\n\n# Impute test data\n# Ensure that all columns in numeric_cols exist in test data\nnumeric_cols_test = [col for col in numeric_cols if col in test.columns]\ntest[numeric_cols_test] = imputer.transform(test[numeric_cols_test])\n\n# Ensure 'sii' remains integer\ntrain['sii'] = train['sii'].round().astype(int)\n\n# ==============================================\n# Model Training and Evaluation Functions\n# ==============================================\ndef quadratic_weighted_kappa(y_true, y_pred):\n    \"\"\"\n    Calculate Quadratic Weighted Kappa.\n    \"\"\"\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    \"\"\"\n    Apply thresholds to continuous predictions.\n    \"\"\"\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    \"\"\"\n    Objective function for optimizing thresholds.\n    \"\"\"\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    \"\"\"\n    Train the model using Stratified K-Fold cross-validation and evaluate it.\n    \"\"\"\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    # Ensure all data is numeric\n    X = X.apply(pd.to_numeric, errors='coerce')\n    test_data = test_data.apply(pd.to_numeric, errors='coerce')\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    # Optimize thresholds\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {tKappa:.3f}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission\n\n# ==============================================\n# Model Parameters and Instantiation\n# ==============================================\n# Define model parameters\nLGBMParams = {\n    'learning_rate': 0.05,\n    'max_depth': 5,\n    'num_leaves': 100,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,\n    'lambda_l2': 1,\n    'random_state': SEED,\n    'verbose': -1,\n    'n_estimators': 100\n}\n\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 5,\n    'n_estimators': 100,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,\n    'reg_lambda': 10,\n    'random_state': SEED\n}\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 5,\n    'iterations': 100,\n    'random_state': SEED,\n    'verbose': 0,\n    'l2_leaf_reg': 10\n}\n\n# Instantiate models\nLight = LGBMRegressor(**LGBMParams)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n\n# Combine models using Voting Regressor\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n])\n\n# ==============================================\n# Train Model and Generate Submission\n# ==============================================\n# Train the ensemble model\nsubmission5 = TrainML(voting_model, test)\n\n# Save submission\n# submission5.to_csv('submission.csv', index=False)\n# print(\"Submission file 'submission.csv' has been created.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\n\n# Load your three submission files\nsub1 = Submission\nsub2 = Submission1\nsub3 = sample\nsub4 = submission4\nsub5 = submission5\n\n# Ensure the IDs are aligned (if not sorted)\nsub1 = sub1.sort_values(by='id').reset_index(drop=True)\nsub2 = sub2.sort_values(by='id').reset_index(drop=True)\nsub3 = sub3.sort_values(by='id').reset_index(drop=True)\nsub4 = sub4.sort_values(by='id').reset_index(drop=True)\nsub5 = sub5.sort_values(by='id').reset_index(drop=True)\n\n# Combine the three predictions\ncombined = pd.DataFrame({\n    'id': sub1['id'],\n    'sii_1': sub1['sii'],\n    'sii_2': sub2['sii'],\n    'sii_3': sub3['sii'],\n    'sii_4': sub4['sii'],\n    'sii_5': sub5['sii']\n})\n\n# Apply majority voting\ndef majority_vote(row):\n    return row.mode()[0]  # Mode gets the most frequent value (majority vote)\n\ncombined['final_sii'] = combined[['sii_1', 'sii_2', 'sii_3', 'sii_4', 'sii_5']].apply(majority_vote, axis=1)\n\n# Create the final submission DataFrame\nfinal_submission = combined[['id', 'final_sii']].rename(columns={'final_sii': 'sii'})\n\n# Save the final submission to a CSV file\nfinal_submission.to_csv('submission.csv', index=False)\n\nprint(\"Majority voting completed and saved to 'submission.csv'\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"final_submission","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}