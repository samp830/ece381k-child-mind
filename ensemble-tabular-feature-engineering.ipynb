{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":81933,"databundleVersionId":9643020,"sourceType":"competition"}],"dockerImageVersionId":30786,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================\n# Import Necessary Libraries\n# ==============================================\nimport os\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom IPython.display import clear_output\nimport warnings\n\nfrom sklearn.base import clone\nfrom sklearn.metrics import cohen_kappa_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom scipy.optimize import minimize\nfrom sklearn.impute import KNNImputer\n\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import VotingRegressor\n\n# Suppress warnings and set display options\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n\n# Set random seed and number of folds for cross-validation\nSEED = 42\nn_splits = 10\n\n# ==============================================\n# Load and Merge Data\n# ==============================================\n# Load main datasets\ntrain = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/train.csv')\ntest = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/test.csv')\nsample = pd.read_csv('/kaggle/input/child-mind-institute-problematic-internet-use/sample_submission.csv')\n\n# ==============================================\n# Feature Engineering\n# ==============================================\ndef engineer_features(df):\n    \"\"\"\n    Create interaction features.\n    \"\"\"\n    df['BMI_Age'] = df['Physical-BMI'] * df['Basic_Demos-Age']\n    df['Internet_Hours_Age'] = df['PreInt_EduHx-computerinternet_hoursday'] * df['Basic_Demos-Age']\n    df['BMI_Internet_Hours'] = df['Physical-BMI'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['Pulse_Pressure'] = df['Physical-Systolic_BP'] - df['Physical-Diastolic_BP']\n    df['HeartRate_Age'] = df['Physical-HeartRate'] * df['Basic_Demos-Age']\n    df['Fitness_Score'] = df['Fitness_Endurance-Max_Stage'] * (\n        df['Fitness_Endurance-Time_Mins'] * 60 + df['Fitness_Endurance-Time_Sec']\n    )\n    df['FMI_FFMI_Ratio'] = df['BIA-BIA_FMI'] / (df['BIA-BIA_FFMI'] + 1e-6)\n    df['Sleep_Internet_Hours'] = df['SDS-SDS_Total_T'] * df['PreInt_EduHx-computerinternet_hoursday']\n    df['Waist_Height_Ratio'] = df['Physical-Waist_Circumference'] / (df['Physical-Height'] + 1e-6)\n    return df\n\n# Apply feature engineering to both train and test datasets\ntrain = engineer_features(train)\ntest = engineer_features(test)\n\n# ==============================================\n# Define Feature Columns\n# ==============================================\n# Define featuresCols as the common columns between train and test\nfeaturesCols = list(set(train.columns).intersection(set(test.columns)))\n\n# Select the features from train and test datasets\ntrain = train[featuresCols + ['sii']]  # Include 'sii' in train features\ntest = test[featuresCols]\n\n# Drop 'id' from train and test data if present\ntrain = train.drop('id', axis=1, errors='ignore')\ntest = test.drop('id', axis=1, errors='ignore')\n\n# Drop rows with missing target variable 'sii' in train data\ntrain = train.dropna(subset=['sii'])\n\n# ==============================================\n# Handle Categorical Variables\n# ==============================================\n# List of categorical columns\ncat_c = [\n    'Basic_Demos-Enroll_Season', 'CGAS-Season', 'Physical-Season', \n    'Fitness_Endurance-Season', 'FGC-Season', 'BIA-Season', \n    'PAQ_A-Season', 'PAQ_C-Season', 'SDS-Season', 'PreInt_EduHx-Season'\n]\n\ndef update_categorical(df):\n    \"\"\"\n    Fill missing values and convert columns to categorical.\n    \"\"\"\n    for c in cat_c: \n        df[c] = df[c].fillna('Missing')\n        df[c] = df[c].astype('category')\n    return df\n\n# Update categorical columns in both datasets\ntrain = update_categorical(train)\ntest = update_categorical(test)\n\ndef create_mapping(column, dataset):\n    \"\"\"\n    Create a mapping for categorical variables.\n    \"\"\"\n    unique_values = dataset[column].unique()\n    return {value: idx for idx, value in enumerate(unique_values)}\n\n# Map categorical variables to integers\nfor col in cat_c:\n    mapping_train = create_mapping(col, train)\n    mapping_test = create_mapping(col, test)\n    \n    train[col] = train[col].replace(mapping_train).astype(int)\n    test[col] = test[col].replace(mapping_test).astype(int)\n\n# ==============================================\n# Handle Missing Values with KNN Imputer\n# ==============================================\n# Impute missing values using KNNImputer\nimputer = KNNImputer(n_neighbors=5)\nnumeric_cols = train.select_dtypes(include=['float64', 'int64']).columns.tolist()\n\n# Remove 'sii' from numeric_cols when applying to test data\nif 'sii' in numeric_cols:\n    numeric_cols.remove('sii')\n\n# Impute train data\ntrain[numeric_cols] = imputer.fit_transform(train[numeric_cols])\n\n# Impute test data\n# Ensure that all columns in numeric_cols exist in test data\nnumeric_cols_test = [col for col in numeric_cols if col in test.columns]\ntest[numeric_cols_test] = imputer.transform(test[numeric_cols_test])\n\n# Ensure 'sii' remains integer\ntrain['sii'] = train['sii'].round().astype(int)\n\n# ==============================================\n# Model Training and Evaluation Functions\n# ==============================================\ndef quadratic_weighted_kappa(y_true, y_pred):\n    \"\"\"\n    Calculate Quadratic Weighted Kappa.\n    \"\"\"\n    return cohen_kappa_score(y_true, y_pred, weights='quadratic')\n\ndef threshold_Rounder(oof_non_rounded, thresholds):\n    \"\"\"\n    Apply thresholds to continuous predictions.\n    \"\"\"\n    return np.where(oof_non_rounded < thresholds[0], 0,\n                    np.where(oof_non_rounded < thresholds[1], 1,\n                             np.where(oof_non_rounded < thresholds[2], 2, 3)))\n\ndef evaluate_predictions(thresholds, y_true, oof_non_rounded):\n    \"\"\"\n    Objective function for optimizing thresholds.\n    \"\"\"\n    rounded_p = threshold_Rounder(oof_non_rounded, thresholds)\n    return -quadratic_weighted_kappa(y_true, rounded_p)\n\ndef TrainML(model_class, test_data):\n    \"\"\"\n    Train the model using Stratified K-Fold cross-validation and evaluate it.\n    \"\"\"\n    X = train.drop(['sii'], axis=1)\n    y = train['sii']\n\n    # Ensure all data is numeric\n    X = X.apply(pd.to_numeric, errors='coerce')\n    test_data = test_data.apply(pd.to_numeric, errors='coerce')\n\n    SKF = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=SEED)\n    \n    train_S = []\n    test_S = []\n    \n    oof_non_rounded = np.zeros(len(y), dtype=float) \n    oof_rounded = np.zeros(len(y), dtype=int) \n    test_preds = np.zeros((len(test_data), n_splits))\n\n    for fold, (train_idx, test_idx) in enumerate(tqdm(SKF.split(X, y), desc=\"Training Folds\", total=n_splits)):\n        X_train, X_val = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_val = y.iloc[train_idx], y.iloc[test_idx]\n\n        model = clone(model_class)\n        model.fit(X_train, y_train)\n\n        y_train_pred = model.predict(X_train)\n        y_val_pred = model.predict(X_val)\n\n        oof_non_rounded[test_idx] = y_val_pred\n        y_val_pred_rounded = y_val_pred.round(0).astype(int)\n        oof_rounded[test_idx] = y_val_pred_rounded\n\n        train_kappa = quadratic_weighted_kappa(y_train, y_train_pred.round(0).astype(int))\n        val_kappa = quadratic_weighted_kappa(y_val, y_val_pred_rounded)\n\n        train_S.append(train_kappa)\n        test_S.append(val_kappa)\n        \n        test_preds[:, fold] = model.predict(test_data)\n        \n        print(f\"Fold {fold+1} - Train QWK: {train_kappa:.4f}, Validation QWK: {val_kappa:.4f}\")\n        clear_output(wait=True)\n\n    print(f\"Mean Train QWK --> {np.mean(train_S):.4f}\")\n    print(f\"Mean Validation QWK ---> {np.mean(test_S):.4f}\")\n\n    # Optimize thresholds\n    KappaOPtimizer = minimize(evaluate_predictions,\n                              x0=[0.5, 1.5, 2.5], args=(y, oof_non_rounded), \n                              method='Nelder-Mead')\n    assert KappaOPtimizer.success, \"Optimization did not converge.\"\n    \n    oof_tuned = threshold_Rounder(oof_non_rounded, KappaOPtimizer.x)\n    tKappa = quadratic_weighted_kappa(y, oof_tuned)\n\n    print(f\"----> || Optimized QWK SCORE :: {tKappa:.3f}\")\n\n    tpm = test_preds.mean(axis=1)\n    tpTuned = threshold_Rounder(tpm, KappaOPtimizer.x)\n    \n    submission = pd.DataFrame({\n        'id': sample['id'],\n        'sii': tpTuned\n    })\n\n    return submission\n\n# ==============================================\n# Model Parameters and Instantiation\n# ==============================================\n# Define model parameters\nLGBMParams = {\n    'learning_rate': 0.05,\n    'max_depth': 5,\n    'num_leaves': 100,\n    'min_data_in_leaf': 13,\n    'feature_fraction': 0.893,\n    'bagging_fraction': 0.784,\n    'bagging_freq': 4,\n    'lambda_l1': 10,\n    'lambda_l2': 1,\n    'random_seed': SEED,\n    'verbose': -1,\n    'n_estimators': 100\n}\n\nXGB_Params = {\n    'learning_rate': 0.05,\n    'max_depth': 5,\n    'n_estimators': 100,\n    'subsample': 0.8,\n    'colsample_bytree': 0.8,\n    'reg_alpha': 1,\n    'reg_lambda': 10,\n    'random_seed': SEED\n}\n\nCatBoost_Params = {\n    'learning_rate': 0.05,\n    'depth': 5,\n    'iterations': 100,\n    'random_seed': SEED,\n    'verbose': 0,\n    'l2_leaf_reg': 10\n}\n\n# Instantiate models\nLight = LGBMRegressor(**LGBMParams)\nXGB_Model = XGBRegressor(**XGB_Params)\nCatBoost_Model = CatBoostRegressor(**CatBoost_Params)\n\n# Combine models using Voting Regressor\nvoting_model = VotingRegressor(estimators=[\n    ('lightgbm', Light),\n    ('xgboost', XGB_Model),\n    ('catboost', CatBoost_Model)\n])\n\n# ==============================================\n# Train Model and Generate Submission\n# ==============================================\n# Train the ensemble model\nsubmission5 = TrainML(voting_model, test)\n\n# Save submission\n# submission5.to_csv('submission.csv', index=False)\n# print(\"Submission file 'submission.csv' has been created.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-28T01:49:51.001150Z","iopub.execute_input":"2024-11-28T01:49:51.001488Z","iopub.status.idle":"2024-11-28T01:50:08.345577Z","shell.execute_reply.started":"2024-11-28T01:49:51.001457Z","shell.execute_reply":"2024-11-28T01:50:08.344481Z"}},"outputs":[{"name":"stderr","text":"Training Folds: 100%|██████████| 10/10 [00:14<00:00,  1.44s/it]","output_type":"stream"},{"name":"stdout","text":"Mean Train QWK --> 0.5710\nMean Validation QWK ---> 0.3647\n----> || Optimized QWK SCORE :: 0.453\nSubmission file 'submission.csv' has been created.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":2}]}